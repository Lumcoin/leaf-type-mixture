{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "## Experiment\n",
    "The goal of the third experiment is to find the best combination of hyperparameters across a range of different regressor models. The experiment assumes the following:\n",
    "\n",
    "- A time window of 1 year is chosen, starting with 1. April, 2017, thus covering all seasons. April of 2017 was the first whole month with Level-2A imagery and the closest to the recording dates.\n",
    "- Level-2A Sentinel 2 satellite imagery and indices are used as input features, using the reducers and numbers of composites determined by the previous experiment.\n",
    "- The evaluation metric for the models is RMSE (Root Mean Squared Error).\n",
    "\n",
    "The hyperparameter optimization is done using Bayesian Optimization with Optuna. The optuna studies and the best pipeline obects are saved using dill instead of pickle to support lambda functions.\n",
    "\n",
    "At first we train a baseline model using simple data. We use a random forest model with default parameters and use all spectral Level-2A bands (starting with \"B\") averaged across the study timewindow as the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline composite from a Sentinel image with the average across 1 year\n",
    "from ltm.data import sentinel_composite\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import make_scorer, root_mean_squared_error\n",
    "\n",
    "data_path = \"../data/processed/hyperparameter_tuning/data_mean_1.tif\"\n",
    "target_path = \"../data/processed/target.tif\"\n",
    "\n",
    "# Create the composite if it does not exist\n",
    "if not Path(data_path).exists():\n",
    "    Path(data_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    sentinel_composite(\n",
    "        target_path_from=target_path,\n",
    "        data_path_to=data_path,\n",
    "        time_window=(datetime(2017, 4, 1), datetime(2018, 4, 1)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate a default random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from ltm.features import load_raster, drop_nan_rows\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scorer = make_scorer(\n",
    "    root_mean_squared_error\n",
    ")\n",
    "\n",
    "data = load_raster(data_path)\n",
    "target = load_raster(target_path)\n",
    "data, target = drop_nan_rows(data, target)\n",
    "\n",
    "cv_result = cross_validate(\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    data,\n",
    "    target,\n",
    "    cv=5,\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "baseline_score = cv_result[\"test_score\"].mean()\n",
    "\n",
    "print(f\"Baseline RMSE: {baseline_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use optuna with `n_jobs=1` for reproducibility. The number of trials is set to 100 for all decision tree ensembles and 500 for the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric and load the data\n",
    "from sklearn.metrics import make_scorer, root_mean_squared_error\n",
    "from ltm.features import load_raster, drop_nan_rows\n",
    "\n",
    "# Define the metric\n",
    "rmse_scorer = make_scorer(\n",
    "    root_mean_squared_error, greater_is_better=False\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "data_path = \"../data/processed/data.tif\"\n",
    "target_path = \"../data/processed/target.tif\"\n",
    "data = load_raster(data_path)\n",
    "target = load_raster(target_path)\n",
    "\n",
    "# Drop rows with NaN in target\n",
    "data, target = drop_nan_rows(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save folder and wrapper functions\n",
    "from pathlib import Path\n",
    "\n",
    "save_folder = \"../models/\"\n",
    "Path(save_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def suggest_categorical(*args, **kwargs):\n",
    "    return \"suggest_categorical\", args, kwargs\n",
    "\n",
    "\n",
    "def suggest_discrete_uniform(*args, **kwargs):\n",
    "    return \"suggest_discrete_uniform\", args, kwargs\n",
    "\n",
    "\n",
    "def suggest_float(*args, **kwargs):\n",
    "    return \"suggest_float\", args, kwargs\n",
    "\n",
    "\n",
    "def suggest_int(*args, **kwargs):\n",
    "    return \"suggest_int\", args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skelm import ELMRegressor\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "elm_default = ELMRegressor(random_state=42)\n",
    "search_space = [\n",
    "    suggest_float(\"alpha\", 1e-8, 1e5, log=True),\n",
    "    suggest_categorical(\"include_original_features\", [True, False]),\n",
    "    suggest_float(\"n_neurons\", 1, 1000),\n",
    "    suggest_categorical(\"ufunc\", [\"tanh\", \"sigm\", \"relu\", \"lin\"]),\n",
    "    suggest_float(\"density\", 0.01, 0.99),\n",
    "]\n",
    "\n",
    "elm_model, elm_study = hyperparam_search(\n",
    "    elm_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=500,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "elm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "knn_default = KNeighborsRegressor(n_jobs=-1)\n",
    "search_space = [\n",
    "    suggest_int(\"n_neighbors\", 1, 100),\n",
    "    suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "    suggest_categorical(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "]\n",
    "\n",
    "knn_model, knn_study = hyperparam_search(\n",
    "    knn_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=500,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Linear Regression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "sgd_default = SGDRegressor(random_state=42)\n",
    "search_space = [\n",
    "    suggest_categorical(\n",
    "        \"loss\",\n",
    "        [\n",
    "            \"squared_error\",\n",
    "            \"huber\",\n",
    "            \"epsilon_insensitive\",\n",
    "            \"squared_epsilon_insensitive\",\n",
    "        ],\n",
    "    ),\n",
    "    suggest_float(\"alpha\", 1e-6, 1e5, log=True),\n",
    "    suggest_float(\"l1_ratio\", 0, 1),\n",
    "]\n",
    "\n",
    "sgd_model, sgd_study = hyperparam_search(\n",
    "    sgd_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=500,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "sgd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVR\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "svr_default = SVR()\n",
    "search_space = [\n",
    "    suggest_float(\"C\", 1e-5, 1e5, log=True),\n",
    "    suggest_float(\"epsilon\", 1e-5, 1e5, log=True),\n",
    "    suggest_categorical(\"kernel\", [\"poly\", \"rbf\", \"sigmoid\"]),\n",
    "]\n",
    "\n",
    "svr_model, svr_study = hyperparam_search(\n",
    "    svr_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=500,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "svr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "et_default = ExtraTreesRegressor(n_jobs=-1, random_state=42)\n",
    "search_space = [\n",
    "    suggest_int(\"n_estimators\", 1, 200),\n",
    "    suggest_float(\"min_impurity_decrease\", 1e-5, 0.5, log=True),\n",
    "    suggest_categorical(\"criterion\", [\"squared_error\", \"absolute_error\"]),\n",
    "]\n",
    "\n",
    "et_model, et_study = hyperparam_search(\n",
    "    et_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "et_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "hgbr_default = HistGradientBoostingRegressor(random_state=42)\n",
    "search_space = [\n",
    "    suggest_int(\"max_iter\", 100, 1000),\n",
    "    suggest_float(\"learning_rate\", 0.001, 0.5, log=True),\n",
    "    suggest_int(\"max_leaf_nodes\", 2, 1000),\n",
    "    suggest_categorical(\"l2_regularization\", [0, 1e-10, 1e-5, 1e-3, 1e-1, 1]),\n",
    "]\n",
    "\n",
    "hgbr_model, hgbr_study = hyperparam_search(\n",
    "    hgbr_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "hgbr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "rf_default = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "search_space = [\n",
    "    suggest_int(\"n_estimators\", 1, 200),\n",
    "    suggest_int(\"max_depth\", 1, 1000),\n",
    "    suggest_float(\"max_features\", 0.1, 1.0),\n",
    "    suggest_float(\"min_samples_split\", 1e-5, 0.5, log=True),\n",
    "    suggest_float(\"min_samples_leaf\", 1e-5, 0.5, log=True),\n",
    "    suggest_categorical(\"bootstrap\", [True, False]),\n",
    "    suggest_categorical(\n",
    "        \"criterion\",\n",
    "        [\"squared_error\", \"absolute_error\", \"poisson\", \"friedman_mse\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "rf_model, rf_study = hyperparam_search(\n",
    "    rf_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from ltm.models import hyperparam_search\n",
    "\n",
    "xgb_default = XGBRegressor(n_jobs=-1, random_state=42)\n",
    "search_space = [\n",
    "    suggest_int(\"n_estimators\", 10, 200),\n",
    "    suggest_int(\"max_depth\", 1, 20),\n",
    "    suggest_float(\"learning_rate\", 0.001, 0.5, log=True),\n",
    "    suggest_float(\"gamma\", 0, 0.5),\n",
    "    suggest_int(\"min_child_weight\", 1, 11),\n",
    "]\n",
    "\n",
    "xgb_model, xgb_study = hyperparam_search(\n",
    "    xgb_default,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    rmse_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the hyperparam search scores during search\n",
    "studies = [\n",
    "    elm_study,\n",
    "    knn_study,\n",
    "    sgd_study,\n",
    "    svr_study,\n",
    "    et_study,\n",
    "    hgbr_study,\n",
    "    rf_study,\n",
    "    xgb_study,\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for ax, study in zip(axs.flat, studies):\n",
    "    trials = study.trials_dataframe()\n",
    "    ax.plot(-trials[\"value\"], label=\"Score\")\n",
    "    ax.set_title(study.study_name)\n",
    "    ax.set_xlabel(\"Trial\")\n",
    "    ax.set_ylabel(\"RMSE Score\")\n",
    "    ax.axhline(-study.best_value, color=\"g\", linestyle=\"--\", label=\"Best score\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "csv_path = \"../reports/hyperparameter_tuning.csv\"\n",
    "scoring = {\n",
    "    \"Mean Absolute Error\": make_scorer(mean_absolute_error),\n",
    "    \"Root Mean Squared Error\": make_scorer(root_mean_squared_error),\n",
    "    \"R2 Score\": make_scorer(r2_score),\n",
    "}\n",
    "\n",
    "tuned_models = [\n",
    "    elm_model,\n",
    "    knn_model,\n",
    "    sgd_model,\n",
    "    svr_model,\n",
    "    rf_model,\n",
    "    et_model,\n",
    "    hgbr_model,\n",
    "    xgb_model,\n",
    "]\n",
    "\n",
    "if not Path(csv_path).exists():\n",
    "    # Create columns\n",
    "    model_names = [model.steps[-1][1].__class__.__name__ for model in tuned_models]\n",
    "    columns = defaultdict(list)\n",
    "    columns[\"Model\"] = model_names\n",
    "\n",
    "    # Cross validate default and tuned models\n",
    "    for model in tqdm(tuned_models):\n",
    "        cv_result = cross_validate(\n",
    "            model, data, target, scoring=scoring, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        for metric in scoring.keys():\n",
    "            columns[metric].append(cv_result[f\"test_{metric}\"].mean())\n",
    "\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(columns)\n",
    "    df.set_index(\"Model\", inplace=True)\n",
    "    df.to_csv(csv_path)\n",
    "else:\n",
    "    df = pd.read_csv(csv_path, index_col=\"Model\")\n",
    "\n",
    "# Sort the dataframe by RMSE\n",
    "df.sort_values(\"Root Mean Squared Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is the XGBRegressor with a RMSE of 0.251. We visualize the quality of the predictions by predicting the study area using cross validation and plotting the predictions against the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth\n",
    "import rasterio\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Prepare the colormap\n",
    "cmap = mpl.cm.viridis\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "mappable = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Create subplot and colorbar\n",
    "fig, ax = plt.subplots()\n",
    "fig.colorbar(mappable, ax=ax, label=\"Conifer Proportion\", shrink=0.8)\n",
    "\n",
    "# Load the target raster\n",
    "with rasterio.open(target_path) as src:\n",
    "    target_raster = src.read()\n",
    "\n",
    "# Plot the target raster\n",
    "ax.imshow(target_raster.transpose(1, 2, 0), interpolation=\"nearest\", cmap=cmap, norm=norm)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Ground Truth\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltm.models import cv_predict\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 3))\n",
    "fig.colorbar(mappable, ax=axs, label=\"Conifer Proportion\", shrink=0.8)\n",
    "\n",
    "prediction_rasters = []\n",
    "for ax, model in zip(axs.flat, tuned_models):\n",
    "    prediction_raster = cv_predict(model, data_path, target_path)\n",
    "    prediction_rasters.append(prediction_raster)\n",
    "    \n",
    "    im = ax.imshow(prediction_raster.transpose(1, 2, 0), interpolation=\"nearest\", cmap=cmap, norm=norm)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(model.steps[-1][1].__class__.__name__)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction raster of XGBoost to target raster\n",
    "plt.plot(target_raster.flatten(), prediction_rasters[-1].flatten(), \".\", alpha=0.05)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"k\", alpha=0.5)\n",
    "\n",
    "plt.title(\"XGBoost\")\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
