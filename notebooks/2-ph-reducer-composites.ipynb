{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducers\n",
    "\n",
    "## Experiment\n",
    "The goal of the second experiment is to find the best number of composites in conjunction with the best temporal reducers that are beneficial in predicting leaf type mixture as a regression problem. The experiment assumes the following:\n",
    "\n",
    "- A time window of 1 year is chosen, starting with 1. April, 2017, thus covering all seasons. April of 2017 was the first whole month with Level-2A imagery and the closest to the recording dates.\n",
    "- Level-2A Sentinel 2 satellite imagery and indices determined by the previous experiment are used as input features.\n",
    "- Random Forest from scikit-learn with default parameters is chosen as the regression model.\n",
    "- The evaluation metrics for the model are RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), and R2 (Coefficient of Determination).\n",
    "\n",
    "The experiment is conducted on 1 to 12 composites in 1 step intervals with the following temporal reducers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the available reducers (except for reducers with a NonNull variant)\n",
    "from ltm.data import list_reducers\n",
    "\n",
    "reducers = list_reducers()\n",
    "reducers = [\n",
    "    reducer for reducer in reducers if not reducer + \"NonNull\" in reducers\n",
    "]\n",
    "\n",
    "reducers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Sentinel 2 bands and indices determined in the previous experiment are used as input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the bands determined by the previous experiment\n",
    "from ltm.models import bands_from_importance\n",
    "\n",
    "sentinel_bands, indices = bands_from_importance(\n",
    "    \"../reports/band_importance.csv\"\n",
    ")\n",
    "\n",
    "sentinel_bands, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from ltm.data import sentinel_composite\n",
    "from ltm.features import (\n",
    "    load_raster,\n",
    "    interpolate_data,\n",
    "    drop_nan_rows,\n",
    "    to_float32,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import make_scorer, root_mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "search_space = {\n",
    "    \"time_window\": [(datetime(2017, 4, 1), datetime(2018, 4, 1))],\n",
    "    \"num_composites\": list(range(1, 13))[::-1],  # Reverse to start with slowest\n",
    "    \"temporal_reducers\": [[reducer] for reducer in reducers],\n",
    "    \"indices\": [indices],\n",
    "    \"sentinel_bands\": [sentinel_bands],\n",
    "}\n",
    "scoring = {\n",
    "    \"Root Mean Squared Error\": make_scorer(root_mean_squared_error)\n",
    "}\n",
    "target_path = \"../data/processed/target.tif\"\n",
    "\n",
    "\n",
    "def create_data(target_path, search_point):\n",
    "    # Create data_path and ensure directory exists\n",
    "    reducer = search_point[\"temporal_reducers\"][0]\n",
    "    composites = search_point[\"num_composites\"]\n",
    "    data_path_to = (\n",
    "        f\"../data/processed/reducer_composites/data_{reducer}_{composites}.tif\"\n",
    "    )\n",
    "    Path(data_path_to).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create the composite (retrying if it fails)\n",
    "    if not Path(data_path_to).exists():\n",
    "        sleep_time = 60 * 5  # 5 minutes\n",
    "        while True:\n",
    "            try:\n",
    "                sentinel_composite(\n",
    "                    target_path_from=target_path,\n",
    "                    data_path_to=data_path_to,\n",
    "                    **search_point,\n",
    "                )\n",
    "                break\n",
    "            except Exception:\n",
    "                sleep(sleep_time)\n",
    "                sleep_time *= 2\n",
    "                pass\n",
    "\n",
    "    # Fill missing values by linear (circular) interpolation\n",
    "    data = load_raster(data_path_to)\n",
    "    interpolated_data = interpolate_data(data)\n",
    "    interpolated_data = to_float32(interpolated_data)\n",
    "\n",
    "    return interpolated_data\n",
    "\n",
    "\n",
    "def check_row_exists(search_point):\n",
    "    try:\n",
    "        df = pd.read_csv(\"../reports/reducer_composites.csv\")\n",
    "        for i, row in df.iterrows():\n",
    "            if list(row[:2]) == [\n",
    "                search_point[\"temporal_reducers\"][0],\n",
    "                search_point[\"num_composites\"],\n",
    "            ]:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def callback(scores, scoring, search_point):\n",
    "    # Read the reducer_composites.csv\n",
    "    reducer_composites = \"../reports/reducer_composites.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(reducer_composites)\n",
    "    except FileNotFoundError:\n",
    "        columns = [\"Reducer\", \"Composites\"] + list(scoring.keys())\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Store the scores in the dataframe\n",
    "    first_columns = [\n",
    "        search_point[\"temporal_reducers\"][0],\n",
    "        search_point[\"num_composites\"],\n",
    "    ]\n",
    "    last_columns = [scores[f\"test_{key}\"].mean() for key in scoring.keys()]\n",
    "    idx = len(df)\n",
    "    for i, row in df.iterrows():\n",
    "        if list(row[:2]) == first_columns:\n",
    "            idx = i\n",
    "    df.loc[idx] = first_columns + last_columns\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(reducer_composites, index=False)\n",
    "\n",
    "\n",
    "def grid_search(\n",
    "    model,\n",
    "    search_space,\n",
    "    scoring,\n",
    "    target_path,\n",
    "    create_data=create_data,\n",
    "    callback=callback,\n",
    "):\n",
    "    target = load_raster(target_path)\n",
    "\n",
    "    params = search_space.keys()\n",
    "    for values in tqdm(list(product(*search_space.values()))):\n",
    "        search_point = dict(zip(params, values))\n",
    "\n",
    "        # Skip if the row already exists\n",
    "        if check_row_exists(search_point):\n",
    "            continue\n",
    "\n",
    "        data = create_data(target_path, search_point)\n",
    "        if len(data.dropna(axis=1).columns) > 0:\n",
    "            # Kendall p-value is not computed by GEE:\n",
    "            # https://notebook.community/naru-T/2017FOSS4GHOKKAIDO/FOSS4G_Hokkaido\n",
    "            # https://gis.stackexchange.com/questions/418431/significant-mann-kendall-tau\n",
    "            data.dropna(\n",
    "                axis=1, inplace=True\n",
    "            )\n",
    "        data_no_nan, target_no_nan = drop_nan_rows(data, target)\n",
    "\n",
    "        if len(data_no_nan) < 5:\n",
    "            scores = {\n",
    "                f\"test_{key}\": np.full(1, np.nan) for key in scoring.keys()\n",
    "            }\n",
    "        else:\n",
    "            scores = cross_validate(\n",
    "                model, data_no_nan, target_no_nan, scoring=scoring, n_jobs=-1\n",
    "            )\n",
    "        callback(scores, scoring, search_point)\n",
    "\n",
    "\n",
    "grid_search(model, search_space, scoring, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use science style\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: improve plot\n",
    "# Draw one multiline plot per metric with all gray lines except for the 5 best reducers\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../reports/reducer_composites.csv\")\n",
    "\n",
    "dfs = {}\n",
    "for i, (metric, scorer) in enumerate(scoring.items()):\n",
    "    new_df = pd.DataFrame(index=df[\"Composites\"].unique()[::-1])\n",
    "\n",
    "    grouped = df[[\"Reducer\", \"Composites\", metric]].groupby(\"Reducer\")\n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values(\"Composites\")\n",
    "        new_df[name] = np.asarray(group[metric])\n",
    "\n",
    "    dfs[metric] = new_df\n",
    "\n",
    "# fig, axes = plt.subplots(len(scoring), 1, figsize=(5, 5*len(scoring)))\n",
    "ax = plt.subplot()\n",
    "\n",
    "metric = list(dfs.keys())[0]\n",
    "dfs[metric].plot(ax=ax, title=metric, legend=False)\n",
    "\n",
    "golden_ratio = (1 + 5**0.5) / 2\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 1 / golden_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltm.models import plot_report\n",
    "\n",
    "plt_df = pd.DataFrame(\n",
    "    {\"Root Mean Squared Error\": dfs[\"Root Mean Squared Error\"].min()}\n",
    ")\n",
    "\n",
    "ax = plot_report(\n",
    "    plt_df.sort_values(\"Root Mean Squared Error\", ascending=False),\n",
    "    \"Best Scores of Temporal Reducers\",\n",
    "    \"Temporal Reducer\",\n",
    "    \"Score\",\n",
    "    label_rotation=90,\n",
    "    figsize=(6, 2),\n",
    "    marker=\".\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best RMSE for the target without any information\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from ltm.features import load_raster, drop_nan_rows\n",
    "from sklearn.metrics import make_scorer, root_mean_squared_error\n",
    "\n",
    "target = load_raster(target_path)\n",
    "target = drop_nan_rows(target)\n",
    "data = np.zeros_like(target).reshape(-1, 1)\n",
    "scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "dummy = DummyRegressor(strategy=\"mean\")\n",
    "cv_result = cross_validate(dummy, data, target, scoring=scorer, n_jobs=-1)\n",
    "dummy_score = cv_result[\"test_score\"].mean()\n",
    "dummy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of the Results\n",
    "\n",
    "We decide to discard following reducers:\n",
    "- quantitative and logical reducers that only consider the presence/absence of images, but not the actual values of the pixels, which are\n",
    "- all variance and stdDev reducers, with the exception of sampleVariance. They all correlate highly, circular does not make much sense as the data is not in radians, without \"sample\" prefix, it is not as accurate for small sample sizes. Variance performed better than stdDev in most cases.\n",
    "- firstNonNull and lastNonNull reducers, which are highly dependent on the weather conditions on the first and last image respectively.\n",
    "- circular reducers, which are not applicable to the bands of the Sentinel-2 satellite imagery, as the intensities are not radians angles.\n",
    "- kendallsCorrelation as it is NaN\n",
    "- statistical reducers for population statistics, where a sample version exists\n",
    "- \"sum\" is highly correlated with \"mean\"\n",
    "- \"product\" is, like \"sum\", sensitively depend on the number of available images for a composite\n",
    "- MinMax as it combines min and max, which are already present as reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all reducers that come into question\n",
    "reducers = [\n",
    "    \"median\",\n",
    "    \"mean\",\n",
    "    \"mode\",\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "    \"sampleVariance\",\n",
    "    \"kendallsCorrelation\",\n",
    "    \"skew\",\n",
    "    \"kurtosis\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best number of composites for each reducer\n",
    "best_composites = pd.DataFrame()\n",
    "\n",
    "metric = list(scoring.keys())[0]\n",
    "best_composites[\"Smallest Number of Composites\"] = dfs[metric][\n",
    "    reducers\n",
    "].idxmin()\n",
    "\n",
    "best_composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltm.features import save_raster\n",
    "\n",
    "total_data_path = \"../data/processed/data.tif\"\n",
    "\n",
    "if not Path(total_data_path).exists():\n",
    "    total_data = pd.DataFrame()\n",
    "\n",
    "    for reducer in tqdm(best_composites.index):\n",
    "        num_composites = best_composites.loc[\n",
    "            reducer, \"Smallest Number of Composites\"\n",
    "        ]\n",
    "        data_path = f\"../data/processed/reducer_composites/data_{reducer}_{num_composites}.tif\"\n",
    "        Path(data_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not Path(data_path).exists():\n",
    "            sentinel_composite(\n",
    "                target_path_from=target_path,\n",
    "                data_path_to=data_path,\n",
    "                time_window=(datetime(2017, 4, 1), datetime(2018, 4, 1)),\n",
    "                temporal_reducers=[reducer],\n",
    "                num_composites=int(num_composites),\n",
    "                indices=indices,\n",
    "                sentinel_bands=sentinel_bands,\n",
    "            )\n",
    "\n",
    "        data = load_raster(data_path)\n",
    "        data = interpolate_data(data)\n",
    "        data.dropna(axis=1, inplace=True)  # Kendall's Correlation P-Value\n",
    "        data = to_float32(data)\n",
    "        total_data = pd.concat([total_data, data], axis=1)\n",
    "\n",
    "    save_raster(total_data, \"../data/processed/target.tif\", total_data_path)\n",
    "else:\n",
    "    total_data = load_raster(total_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the new data\n",
    "total_data = load_raster(\"../data/processed/data.tif\")\n",
    "target = load_raster(target_path)\n",
    "total_data, target = drop_nan_rows(total_data, target)\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(\n",
    "    model, total_data, target, scoring=scoring, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Compute the means per metric\n",
    "means = {f\"test_{key}\": scores[f\"test_{key}\"].mean() for key in scoring.keys()}\n",
    "means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
