{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this experiment is to find the best combination of hyperparameters across a range of different regressor models and select the best model.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "We conduct the experiment with following decisions:\n",
    "\n",
    "- A time window of 1 year is used per study area, covering each year of recording.\n",
    "- We use the dataset created in the previous experiment.\n",
    "- We perform hyperparameter tuning for the following classifiers:\n",
    "  - Extreme Learning Machine\n",
    "  - K-Nearest Neighbors\n",
    "  - Linear Regression fitted with Stochastic Gradient Descent\n",
    "  - Support Vector Machine\n",
    "  - Extra Trees\n",
    "  - Histogram-based Gradient Boosting\n",
    "  - Random Forest\n",
    "  - eXtreme Gradient Boosting\n",
    "- Bayesian Optimization with Optuna is used to find the best hyperparameters, using F1 score with stratified 5-fold cross-validation per iteration as the objective.\n",
    "- The number of trials is set to 100 for models.\n",
    "\n",
    "We load the dataset and perform the hyperparameter tuning for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric and load the data\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Define the metric\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"../data/processed/compositing/data.csv\")\n",
    "target = pd.read_csv(\"../data/processed/compositing/target.csv\")[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save folder and wrapper functions\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "save_folder = \"../models/\"\n",
    "Path(save_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _suggest_categorical(*args: Any, **kwargs: Any) -> tuple:  # noqa: ANN401\n",
    "    return \"suggest_categorical\", args, kwargs\n",
    "\n",
    "\n",
    "def _suggest_discrete_uniform(*args: Any, **kwargs: Any) -> tuple:  # noqa: ANN401\n",
    "    return \"suggest_discrete_uniform\", args, kwargs\n",
    "\n",
    "\n",
    "def _suggest_float(*args: Any, **kwargs: Any) -> tuple:  # noqa: ANN401\n",
    "    return \"suggest_float\", args, kwargs\n",
    "\n",
    "\n",
    "def _suggest_int(*args: Any, **kwargs: Any) -> tuple:  # noqa: ANN401\n",
    "    return \"suggest_int\", args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Learning Machine\n",
    "from logging import INFO\n",
    "from sys import stdout\n",
    "\n",
    "from loguru import logger\n",
    "from skelm import ELMClassifier\n",
    "from slc.models import hyperparam_search\n",
    "\n",
    "logger.configure(handlers=[{\"sink\": stdout, \"format\": \"{message}\", \"level\": INFO}])\n",
    "\n",
    "elm_model = ELMClassifier(random_state=42)\n",
    "search_space = [\n",
    "    _suggest_float(\"alpha\", 1e-8, 1e5, log=True),\n",
    "    _suggest_categorical(\"include_original_features\", [True, False]),\n",
    "    _suggest_categorical(\"ufunc\", [\"tanh\", \"sigm\", \"relu\", \"lin\"]),\n",
    "    _suggest_categorical(\"n_neurons\", [1, None]),\n",
    "    _suggest_float(\"density\", 0.01, 0.99),\n",
    "]\n",
    "\n",
    "elm_pipe, elm_study = hyperparam_search(\n",
    "    elm_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "elm_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from slc.models import hyperparam_search\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_jobs=-1)\n",
    "search_space = [\n",
    "    _suggest_int(\"n_neighbors\", 1, 100),\n",
    "    _suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "    _suggest_categorical(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "]\n",
    "\n",
    "knn_pipe, knn_study = hyperparam_search(\n",
    "    knn_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "knn_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier(random_state=42)\n",
    "search_space = [\n",
    "    _suggest_categorical(\n",
    "        \"loss\",\n",
    "        [\n",
    "            \"squared_epsilon_insensitive\",\n",
    "            \"modified_huber\",\n",
    "            \"log_loss\",\n",
    "            \"perceptron\",\n",
    "            \"squared_error\",\n",
    "            \"squared_hinge\",\n",
    "            \"epsilon_insensitive\",\n",
    "            \"huber\",\n",
    "            \"hinge\",\n",
    "        ],\n",
    "    ),\n",
    "    _suggest_float(\"alpha\", 1e-6, 1e5, log=True),\n",
    "    _suggest_float(\"l1_ratio\", 0, 1),\n",
    "    _suggest_int(\"max_iter\", 1, 10000),\n",
    "]\n",
    "\n",
    "sgd_pipe, sgd_study = hyperparam_search(\n",
    "    sgd_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "sgd_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC()\n",
    "search_space = [\n",
    "    _suggest_float(\"C\", 1e-5, 1e5, log=True),\n",
    "    _suggest_categorical(\"kernel\", [\"poly\", \"rbf\", \"sigmoid\"]),\n",
    "]\n",
    "\n",
    "svm_pipe, svm_study = hyperparam_search(\n",
    "    svm_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    "    always_standardize=True,\n",
    ")\n",
    "\n",
    "svm_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_model = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "search_space = [\n",
    "    _suggest_int(\"n_estimators\", 1, 200),\n",
    "    _suggest_float(\"min_impurity_decrease\", 1e-5, 0.5, log=True),\n",
    "    _suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"]),\n",
    "]\n",
    "\n",
    "et_pipe, et_study = hyperparam_search(\n",
    "    et_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "et_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram-based Gradient Boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb_model = HistGradientBoostingClassifier(random_state=42)\n",
    "search_space = [\n",
    "    _suggest_int(\"max_iter\", 10, 1000),\n",
    "    _suggest_float(\"learning_rate\", 0.001, 0.5, log=True),\n",
    "    _suggest_int(\"max_leaf_nodes\", 2, 1000),\n",
    "    _suggest_categorical(\"l2_regularization\", [0, 1e-10, 1e-5, 1e-3, 1e-1, 1]),\n",
    "]\n",
    "\n",
    "hgb_pipe, hgb_study = hyperparam_search(\n",
    "    hgb_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "hgb_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "search_space = [\n",
    "    _suggest_int(\"n_estimators\", 1, 200),\n",
    "    _suggest_int(\"max_depth\", 1, 1000),\n",
    "    _suggest_float(\"max_features\", 0.1, 1.0),\n",
    "    _suggest_float(\"min_samples_split\", 1e-5, 0.5, log=True),\n",
    "    _suggest_float(\"min_samples_leaf\", 1e-5, 0.5, log=True),\n",
    "    _suggest_categorical(\"bootstrap\", [True, False]),\n",
    "    _suggest_categorical(\n",
    "        \"criterion\",\n",
    "        [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "rf_pipe, rf_study = hyperparam_search(\n",
    "    rf_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eXtreme Gradient Boosting\n",
    "from slc.models import hyperparam_search\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(n_jobs=-1, random_state=42)\n",
    "search_space = [\n",
    "    _suggest_int(\"n_estimators\", 10, 200),\n",
    "    _suggest_int(\"max_depth\", 1, 20),\n",
    "    _suggest_float(\"learning_rate\", 0.001, 0.5, log=True),\n",
    "    _suggest_float(\"gamma\", 0, 0.5),\n",
    "    _suggest_int(\"min_child_weight\", 1, 11),\n",
    "    _suggest_int(\"reg_alpha\", 40, 180),\n",
    "    _suggest_float(\"reg_lambda\", 0, 1),\n",
    "    _suggest_float(\"colsample_bytree\", 0.5, 1),\n",
    "]\n",
    "\n",
    "xgb_pipe, xgb_study = hyperparam_search(\n",
    "    xgb_model,\n",
    "    search_space,\n",
    "    data,\n",
    "    target,\n",
    "    f1_scorer,\n",
    "    n_trials=100,\n",
    "    save_folder=save_folder,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the hyperparam search scores during search\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots  # noqa: F401\n",
    "\n",
    "from slc.visualize import save_fig\n",
    "\n",
    "plt.style.use(\"science\")\n",
    "\n",
    "studies = [\n",
    "    elm_study,\n",
    "    knn_study,\n",
    "    sgd_study,\n",
    "    svm_study,\n",
    "    et_study,\n",
    "    hgb_study,\n",
    "    rf_study,\n",
    "    xgb_study,\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for ax, study in zip(axs.flat, studies, strict=False):\n",
    "    trials = study.trials_dataframe()\n",
    "    ax.plot(trials[\"value\"], label=\"Score\")\n",
    "    ax.set_title(study.study_name)\n",
    "    ax.set_xlabel(\"Trial\")\n",
    "    ax.set_ylabel(\"F1 Score\")\n",
    "    ax.axhline(study.best_value, color=\"g\", linestyle=\"--\", label=\"Best score\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "\n",
    "save_fig(fig, \"../reports/figures/hyperparameter_tuning/Hyperparameter Tuning.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above figure you can see the hyperparameter tuning process for each model. The best F1 Score of each model is between 0.6 and 0.7, while the F1 Scores during the tuning processes vary greatly. For example the K-Nearest Neighbors classifier mostly produces F1 Scores above 0.5 and the Linear Regression (SGDClassifier) frequently approaches and even reaches F1 Scores of 0. We compute the F1 Score, Accuracy and Kappa for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned models\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "csv_path = \"../reports/hyperparameter_tuning.csv\"\n",
    "scoring = {\n",
    "    \"F1 Score\": make_scorer(f1_score),\n",
    "    \"Accuracy\": make_scorer(accuracy_score),\n",
    "    \"Kappa\": make_scorer(cohen_kappa_score),\n",
    "}\n",
    "\n",
    "tuned_models = [\n",
    "    elm_pipe,\n",
    "    knn_pipe,\n",
    "    sgd_pipe,\n",
    "    svm_pipe,\n",
    "    rf_pipe,\n",
    "    et_pipe,\n",
    "    hgb_pipe,\n",
    "    xgb_pipe,\n",
    "]\n",
    "\n",
    "if not Path(csv_path).exists():\n",
    "    # Create columns\n",
    "    model_names = [model.steps[-1][1].__class__.__name__ for model in tuned_models]\n",
    "    columns = defaultdict(list)\n",
    "    columns[\"Model\"] = model_names\n",
    "\n",
    "    # Cross validate default and tuned models\n",
    "    for model in tqdm(tuned_models):\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_result = cross_validate(\n",
    "            model, data, target, cv=cv, scoring=scoring, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        for metric in scoring:\n",
    "            columns[metric].append(cv_result[f\"test_{metric}\"].mean())\n",
    "\n",
    "    # Create dataframe\n",
    "    results = pd.DataFrame(columns)\n",
    "    results = results.set_index(\"Model\")\n",
    "    results.to_csv(csv_path)\n",
    "else:\n",
    "    results = pd.read_csv(csv_path, index_col=\"Model\")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
