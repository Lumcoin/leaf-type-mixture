{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "Preprocess and unify all data.\n",
    "\n",
    "Before starting to train a ML model, we have to preprocess our data. In this case Sentinel-2 Level-2A imagery is used to generate composites by maximum NDVI across a period of two months. The resulting composites are augmented with indices, like NDVI and all timesteps are reduced into a single raster by deriving statistical parameters, like mean and variance.\n",
    "\n",
    "The DEM image uploaded beforehand is downsampled to the same resolution as the Sentinel-2 composites by calculating various textile measures.\n",
    "\n",
    "Then the resulting Sentinel-2 derived raster and DEM derived raster are stacked and a dimensionality reduction is performed. The reduced image can then be used for further processing.\n",
    "\n",
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Earth Engine API and initialize it\n",
    "import ee\n",
    "ee.Initialize()\n",
    "\n",
    "# Define data constants\n",
    "SOURCE = 'COPERNICUS/S2_SR'  # Define dataset source\n",
    "REGION = ee.Geometry.Rectangle([12.6545, 47.9291, 12.6762, 47.9423])  # Define region in EPSG:4326\n",
    "\n",
    "# Define processing constants\n",
    "TIMESERIES_MIDDLE = '2019-06-01'  # Define middle of timeseries\n",
    "TIMESERIES_DURATION = 365  # Define duration of timeseries in days\n",
    "NUM_COMPOSITES = 12  # Define amount of composites in the timeseries\n",
    "TEMPORAL_REDUCERS = [ee.Reducer.median(), ee.Reducer.variance()]  # Define temporal reducer\n",
    "\n",
    "# Define quality measure for composites\n",
    "def addQuality(image):\n",
    "    quality_band = image.normalizedDifference(['B5', 'B4']).rename(['quality'])  # NDVI in this case\n",
    "    return image.addBands(quality_band)\n",
    "\n",
    "# Define export constants\n",
    "FILENAME = 'NDVI_composite'  # Name of exported raster\n",
    "FOLDER = 'Google Earth Engine'  # Name of export folder\n",
    "SCALE = 10  # Size of pixel in meters\n",
    "CRS = 'EPSG:32632'  # Coordinate reference system of exported raster\n",
    "MAX_PIXELS = 1e7  # Maximum number of pixels when exporting\n",
    "\n",
    "# Define map constants\n",
    "VIS_PARAMS = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.2, 'gamma': 1}\n",
    "LAYER_NAME = FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Timeseries Windows for calculating Composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_timewindows(middle_date, num_windows, timeseries_duration=365.242):\n",
    "    # Calculate the start date of the timeseries\n",
    "    middle_date = datetime.strptime(middle_date, '%Y-%m-%d')\n",
    "    current_start = middle_date - timedelta(days=timeseries_duration / 2)\n",
    "\n",
    "    # Calculate the duration of each timewindow (in days)\n",
    "    window_duration = timeseries_duration / num_windows\n",
    "    \n",
    "    # Initialize a list to store the timewindows as tuples\n",
    "    timewindows = []\n",
    "    for _ in range(num_windows):\n",
    "        # Calculate the start and end dates of each timewindow\n",
    "        start_date = current_start\n",
    "        end_date = current_start + timedelta(days=window_duration)\n",
    "\n",
    "        start_date = start_date.strftime('%Y-%m-%d')\n",
    "        end_date = end_date.strftime('%Y-%m-%d')\n",
    "        timewindow = (start_date, end_date)\n",
    "        \n",
    "        # Append the timewindow as a tuple (start, end) to the list\n",
    "        timewindows.append(timewindow)\n",
    "        \n",
    "        # Move the middle_date to the next timewindow\n",
    "        current_start += timedelta(days=window_duration)\n",
    "    \n",
    "    return timewindows\n",
    "\n",
    "timewindows = generate_timewindows(TIMESERIES_MIDDLE, NUM_COMPOSITES, TIMESERIES_DURATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Sentinel-2 (Level-2A) imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskS2clouds(image, cld_thresh=0.8, snw_thresh=0.8):\n",
    "  # -----------------------------------------------------------------------\n",
    "  # qa = image.select('QA60')\n",
    "\n",
    "  # # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "  # cloudBitMask = 1 << 10\n",
    "  # cirrusBitMask = 1 << 11\n",
    "\n",
    "  # # Both flags should be set to zero, indicating clear conditions.\n",
    "  # mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "  # -----------------------------------------------------------------------\n",
    "\n",
    "  # Use MSK_CLDPRB and MSK_SNWPRB bands with a threshold of 100\n",
    "  cld_prb = image.select('MSK_CLDPRB').divide(100)\n",
    "  snw_prb = image.select('MSK_SNWPRB').divide(100)\n",
    "  mask = cld_prb.lt(cld_thresh).And(snw_prb.lt(snw_thresh))\n",
    "\n",
    "  return image.updateMask(mask).divide(10000)\n",
    "\n",
    "def get_bands(image, print=False):\n",
    "  bands = [band['id'] for band in image.getInfo()['bands']]\n",
    "\n",
    "  if print:\n",
    "    print(bands)\n",
    "\n",
    "  return bands\n",
    "\n",
    "def get_reducer_name(reducer, print=False):\n",
    "  reducer_name = reducer.getInfo()['type'].split('.')[-1]\n",
    "\n",
    "  if print:\n",
    "    print(reducer_name)\n",
    "\n",
    "  return reducer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods for calculating indices\n",
    "def add_ndvi(image):\n",
    "    ndvi_band = image.normalizedDifference(['B5', 'B4']).rename(['NDVI'])\n",
    "    return image.addBands(ndvi_band)\n",
    "\n",
    "def add_ndwi(image):\n",
    "    ndwi_band = image.normalizedDifference(['B3', 'B5']).rename(['NDWI'])\n",
    "    return image.addBands(ndwi_band)\n",
    "\n",
    "add_indices = [add_ndvi, add_ndwi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "composites = []\n",
    "for start, end in timewindows:\n",
    "    # Define dataset filters\n",
    "    filter_date = ee.Filter.date(start, end)  # inclusive start, exclusive end\n",
    "    filter_region = ee.Filter.bounds(REGION)\n",
    "\n",
    "    # Read dataset\n",
    "    dataset = ee.ImageCollection(SOURCE).filter(filter_date).filter(filter_region).map(maskS2clouds)\n",
    "    \n",
    "    # Calculate indices\n",
    "    for add_index in add_indices:\n",
    "        dataset = dataset.map(add_index)\n",
    "\n",
    "    # Create max NDVI pixel composite\n",
    "    dataset = dataset.map(addQuality)  # Add quality band\n",
    "    composite = dataset.qualityMosaic('quality')  # Choose max quality pixels\n",
    "\n",
    "    # Remove quality band\n",
    "    remaining_bands = composite.bandNames().getInfo()\n",
    "    remaining_bands.remove('quality')\n",
    "    composite = composite.select(remaining_bands)  # Remove quality band\n",
    "\n",
    "    # Add to time series\n",
    "    composites.append(composite)\n",
    "\n",
    "# Create image collection\n",
    "composites = ee.ImageCollection(composites)\n",
    "\n",
    "# Apply remporal reducers to image collection\n",
    "reduced_images = []\n",
    "for temporal_reducer in TEMPORAL_REDUCERS:\n",
    "    reduced_images.append(composites.reduce(temporal_reducer))\n",
    "\n",
    "# Stack images\n",
    "stacked_images = ee.ImageCollection(reduced_images).toBands()\n",
    "stacked_images = stacked_images.reproject(crs=CRS, scale=SCALE)\n",
    "\n",
    "# Resample and clip to region\n",
    "stacked_images = stacked_images.reproject(crs=CRS, scale=SCALE)\n",
    "stacked_images = stacked_images.clip(REGION)\n",
    "\n",
    "# Define RGB bands\n",
    "rgb_bands = get_bands(stacked_images)[3:0:-1]\n",
    "\n",
    "print('Stacked images:', stacked_images.bandNames().getInfo())\n",
    "\n",
    "# Show image\n",
    "Image(url=stacked_images.getThumbUrl({\n",
    "    **VIS_PARAMS,\n",
    "    'bands': rgb_bands,\n",
    "    'dimensions': 500}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for all bands in the image collection\n",
    "import pandas as pd\n",
    "\n",
    "# Select two bands to calculate correlation coefficient\n",
    "df = pd.DataFrame(stacked_images.reduceRegion(ee.Reducer.toList(), maxPixels=1e8).getInfo())\n",
    "correlation_matrix = df.corr().fillna(1)\n",
    "correlation_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the image as a numpy array\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# url = composites.getDownloadUrl({\n",
    "#     'scale': SCALE,\n",
    "#     'region': REGION,\n",
    "#     'format': 'NPY'\n",
    "#     })\n",
    "\n",
    "# response = requests.get(url)\n",
    "# data = np.load(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Load the DEM\n",
    "dem = ee.Image('projects/leaf-type-mixture/assets/DEM')\n",
    "band_name = dem.bandNames().getInfo()[0]\n",
    "\n",
    "# Calculate textile measures\n",
    "dem_mean = dem.reduceResolution(ee.Reducer.mean(), maxPixels=1024).rename(band_name + '_mean')\n",
    "dem_variance = dem.reduceResolution(ee.Reducer.variance(), maxPixels=1024).rename(band_name + '_variance')\n",
    "dem_glcm = dem.multiply(1000).toUint16().glcmTexture()  # naive approach to avoid memory issues\n",
    "\n",
    "# Stack the DEM and its gradient\n",
    "dem = dem.addBands([dem_mean, dem_variance, dem_glcm])\n",
    "\n",
    "# Clip to the region of interest\n",
    "dem = dem.reproject(crs=CRS, scale=SCALE)\n",
    "dem = dem.clip(REGION)\n",
    "\n",
    "# Print all bands\n",
    "print(dem.bandNames().getInfo())\n",
    "\n",
    "Image(url=dem.getThumbUrl({\n",
    "    'min': -5,\n",
    "    'max': 50,\n",
    "    'bands': ['b1_shade', 'b1_diss', 'b1_prom'],\n",
    "    'region': REGION,\n",
    "    'dimensions': 500}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rasterize Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Constants, zero for conifers, one for broadleafs\n",
    "CONIFER = 0\n",
    "BROADLEAF = 1\n",
    "\n",
    "# Map leaf type to a number\n",
    "leaf_type_dict = {'Abies alba': CONIFER,\n",
    "                  'Acer campestre': BROADLEAF,\n",
    "                  'Acer platanoides': BROADLEAF,\n",
    "                  'Acer pseudoplatanus': BROADLEAF,   \n",
    "                  'Aesculus hippocastanum': BROADLEAF,\n",
    "                  'Alnus glutinose': BROADLEAF,\n",
    "                  'Betula ': BROADLEAF,\n",
    "                  'Carpinus betulus': BROADLEAF,\n",
    "                  'Fagus sylvatica': BROADLEAF,\n",
    "                  'Fraxinus excelsior': BROADLEAF,\n",
    "                  'Juglans regia': BROADLEAF,\n",
    "                  'Larix decidua': CONIFER,\n",
    "                  'Picea abies': CONIFER,\n",
    "                  'Pinus sylvestris': CONIFER,\n",
    "                  'Populus ': BROADLEAF,\n",
    "                  'Populus tremula': BROADLEAF,\n",
    "                  'Prunus avium': BROADLEAF,\n",
    "                  'Pseudotsuga menziesii': CONIFER,\n",
    "                  'Quercus ': BROADLEAF,\n",
    "                  'Quercus rubra': BROADLEAF,\n",
    "                  'Salix ': BROADLEAF,\n",
    "                  'Sorbus aria': BROADLEAF,\n",
    "                  'Sorbus aucuparia': BROADLEAF,\n",
    "                  'Sorbus torminalis': BROADLEAF,\n",
    "                  'Thuja plicata': CONIFER,\n",
    "                  'Tilia ': BROADLEAF,\n",
    "                  'Ulmus glabra': BROADLEAF,\n",
    "                  'Unidentified broadleaf': BROADLEAF,\n",
    "                  'Unidentified conifer': CONIFER}\n",
    "\n",
    "# Read in the plot data\n",
    "df = gpd.read_file('../data/raw/Plot.gpkg').to_crs('EPSG:4326')\n",
    "\n",
    "# Create Longitude, Latitude, and Broadleaf columns\n",
    "df['Longitude'] = df['geometry'].x\n",
    "df['Latitude'] = df['geometry'].y\n",
    "df['Species'] = df['Latin'] + ' ' + df['Mnemonic']\n",
    "df['Broadleaf'] = df['Species'].map(leaf_type_dict)\n",
    "\n",
    "# Keep only the columns we need\n",
    "df = df[['Longitude', 'Latitude', 'Broadleaf']]\n",
    "\n",
    "# Convert pandas DataFrame to Earth Engine FeatureCollection\n",
    "fc = ee.FeatureCollection([ee.Feature(ee.Geometry.Point([row['Longitude'], row['Latitude']]), {'Broadleaf': row['Broadleaf']}) for index, row in df.iterrows()])\n",
    "\n",
    "# Rasterize the FeatureCollection using reduceToImage\n",
    "plot = fc.reduceToImage(['Broadleaf'], ee.Reducer.mean())\n",
    "plot = plot.reproject(crs=CRS, scale=SCALE)\n",
    "plot = plot.clip(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Height Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heightmask = dem.select('b1').gt(20)\n",
    "\n",
    "masked_s2 = stacked_images.updateMask(heightmask)\n",
    "masked_s2 = masked_s2.reproject(crs=CRS, scale=SCALE)\n",
    "masked_s2 = masked_s2.clip(REGION)\n",
    "\n",
    "Image(url=masked_s2.getThumbUrl({\n",
    "    'min': 0,\n",
    "    'max': 0.2,\n",
    "    'bands': ['0_B4_median', '0_B3_median', '0_B2_median'],\n",
    "    'dimensions': 500}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save (& fuse DEM with Time Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import rasterio\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "def download_image(image, scale, crs):\n",
    "    download_params = {\n",
    "        'scale': scale,\n",
    "        'crs': crs,\n",
    "        'format': 'GEO_TIFF',\n",
    "    }\n",
    "    url = image.getDownloadURL(download_params)\n",
    "\n",
    "    return requests.get(url)\n",
    "\n",
    "def save_image_to_file(image, file_path, mask, bands):\n",
    "    with MemoryFile(image) as memfile, MemoryFile(mask) as mask_memfile:\n",
    "        with memfile.open() as dataset, mask_memfile.open() as mask_dataset:\n",
    "            profile = dataset.profile\n",
    "            with rasterio.open(file_path, 'w', **profile) as dst:\n",
    "                raster = dataset.read()\n",
    "                mask = mask_dataset.read()\n",
    "                raster[mask == 0] = np.nan\n",
    "                dst.write(raster)\n",
    "                dst.descriptions = tuple(bands)\n",
    "\n",
    "def save_image(image, file_path, scale, crs):\n",
    "    image_response = download_image(image, scale, crs)\n",
    "    mask_response = download_image(image.mask(), scale, crs)\n",
    "    \n",
    "    if image_response.status_code == mask_response.status_code == 200:\n",
    "        save_image_to_file(image_response.content,\n",
    "                           file_path,\n",
    "                           mask=mask_response.content,\n",
    "                           bands=image.bandNames().getInfo())\n",
    "        print('Image downloaded and saved successfully!')\n",
    "    else:\n",
    "        print('Failed to download the image.')\n",
    "\n",
    "fusion = stacked_images.addBands(dem)\n",
    "\n",
    "# Merge all masks with logical AND operation\n",
    "band_names = fusion.bandNames().getInfo()\n",
    "mask = fusion.select(band_names[0]).mask()\n",
    "for band_name in band_names[1:]:\n",
    "    mask = mask.And(fusion.select(band_name).mask())\n",
    "band_names = plot.bandNames().getInfo()\n",
    "for band_name in band_names:\n",
    "    mask = mask.And(plot.select(band_name).mask())\n",
    "\n",
    "# Apply the mask to both images\n",
    "fusion = fusion.updateMask(mask)\n",
    "plot = plot.updateMask(mask)\n",
    "\n",
    "fusion = fusion.clip(REGION)\n",
    "plot = plot.clip(REGION)\n",
    "\n",
    "# save_image\n",
    "save_image(fusion, 'fusion.tif', scale=SCALE, crs=CRS)\n",
    "save_image(plot, 'plot.tif', scale=SCALE, crs=CRS)\n",
    "\n",
    "# Open the raster dataset\n",
    "input_path = 'fusion.tif'\n",
    "with rasterio.open(input_path) as src:\n",
    "    # read description of the raster\n",
    "    print(src.descriptions)\n",
    "input_path = 'plot.tif'\n",
    "with rasterio.open(input_path) as src:\n",
    "    # read description of the raster\n",
    "    print(src.descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to grayscale\n",
    "gray = stacked_images.select(['0_B4_median', '0_B3_median', '0_B2_median']).reduce(ee.Reducer.mean())  # reduce three bands to one\n",
    "gray = gray.toUint16()\n",
    "\n",
    "# Calculate the GLCM\n",
    "glcm = gray.glcmTexture(3)\n",
    "\n",
    "# Select the texture measures to export\n",
    "texture_measures = glcm.select(\n",
    "    ['mean_contrast', 'mean_diss', 'mean_shade', 'mean_asm', 'mean_prom'])\n",
    "\n",
    "# Reduce the texture measures by mean\n",
    "reduced_texture_measures = texture_measures.reduceRegion(\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    geometry=REGION,\n",
    "    scale=SCALE,\n",
    "    crs=CRS)\n",
    "\n",
    "# Convert to dictionary\n",
    "reduced_texture_measures = reduced_texture_measures.getInfo()\n",
    "reduced_texture_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a map.\n",
    "lon, lat = REGION.centroid().getInfo()['coordinates']\n",
    "# lat, lon = 45.77, 4.855\n",
    "my_map = folium.Map(location=[lat, lon], zoom_start=15)\n",
    "\n",
    "# Define a method for displaying Earth Engine image tiles on a folium map.\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "    \"\"\"Adds a method for displaying Earth Engine image tiles to folium map.\"\"\"\n",
    "    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "    folium.raster_layers.TileLayer(\n",
    "        tiles=map_id_dict['tile_fetcher'].url_format,\n",
    "        attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "        name=name,\n",
    "        overlay=True,\n",
    "        control=True\n",
    "    ).add_to(self)\n",
    "\n",
    "# Add Earth Engine drawing method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer\n",
    "\n",
    "# Add the stacked composites to the map.\n",
    "vis_params = {**VIS_PARAMS, 'bands': ['0_B4_median', '0_B3_median', '0_B2_median']}\n",
    "my_map.add_ee_layer(stacked_images, vis_params, 'Sentinel-2 Composite')\n",
    "\n",
    "# Add the plot to the map.\n",
    "vis_params = {\n",
    "    'min': 0,'max': 1,\n",
    "    'palette': ['05450a','086a10', '54a708', '78d203', '009900', 'c6b044',\n",
    "                'dcd159', 'dade48', 'fbff13', 'b6ff05', '27ff87', 'c24f44',\n",
    "                'a5a5a5', 'ff6d4c', '69fff8', 'f9ffa4', '1c0dff']\n",
    "}\n",
    "my_map.add_ee_layer(plot, vis_params, 'Leaf Type Mixture')\n",
    "\n",
    "# Add a layer control panel to the map.\n",
    "my_map.add_child(folium.LayerControl())\n",
    "\n",
    "# Display the map.\n",
    "display(my_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1929\n",
      "Eval set size: 271\n",
      "Test set size: 271\n",
      "[0]\tvalidation_0-rmse:0.38919\tvalidation_0-mean_squared_error:0.15147\n",
      "[1]\tvalidation_0-rmse:0.35602\tvalidation_0-mean_squared_error:0.12675\n",
      "[2]\tvalidation_0-rmse:0.33938\tvalidation_0-mean_squared_error:0.11518\n",
      "[3]\tvalidation_0-rmse:0.32998\tvalidation_0-mean_squared_error:0.10888\n",
      "[4]\tvalidation_0-rmse:0.32197\tvalidation_0-mean_squared_error:0.10367\n",
      "[5]\tvalidation_0-rmse:0.32103\tvalidation_0-mean_squared_error:0.10306\n",
      "[6]\tvalidation_0-rmse:0.31593\tvalidation_0-mean_squared_error:0.09981\n",
      "[7]\tvalidation_0-rmse:0.31677\tvalidation_0-mean_squared_error:0.10034\n",
      "[8]\tvalidation_0-rmse:0.31391\tvalidation_0-mean_squared_error:0.09854\n",
      "[9]\tvalidation_0-rmse:0.31363\tvalidation_0-mean_squared_error:0.09836\n",
      "[10]\tvalidation_0-rmse:0.31417\tvalidation_0-mean_squared_error:0.09870\n",
      "[11]\tvalidation_0-rmse:0.31376\tvalidation_0-mean_squared_error:0.09845\n",
      "[12]\tvalidation_0-rmse:0.31292\tvalidation_0-mean_squared_error:0.09792\n",
      "[13]\tvalidation_0-rmse:0.31261\tvalidation_0-mean_squared_error:0.09773\n",
      "[14]\tvalidation_0-rmse:0.31392\tvalidation_0-mean_squared_error:0.09855\n",
      "[15]\tvalidation_0-rmse:0.31177\tvalidation_0-mean_squared_error:0.09720\n",
      "[16]\tvalidation_0-rmse:0.31230\tvalidation_0-mean_squared_error:0.09753\n",
      "[17]\tvalidation_0-rmse:0.31316\tvalidation_0-mean_squared_error:0.09807\n",
      "[18]\tvalidation_0-rmse:0.31328\tvalidation_0-mean_squared_error:0.09814\n",
      "[19]\tvalidation_0-rmse:0.31329\tvalidation_0-mean_squared_error:0.09815\n",
      "[20]\tvalidation_0-rmse:0.31069\tvalidation_0-mean_squared_error:0.09653\n",
      "[21]\tvalidation_0-rmse:0.31126\tvalidation_0-mean_squared_error:0.09688\n",
      "[22]\tvalidation_0-rmse:0.31183\tvalidation_0-mean_squared_error:0.09724\n",
      "[23]\tvalidation_0-rmse:0.31095\tvalidation_0-mean_squared_error:0.09669\n",
      "[24]\tvalidation_0-rmse:0.31178\tvalidation_0-mean_squared_error:0.09721\n",
      "[25]\tvalidation_0-rmse:0.31224\tvalidation_0-mean_squared_error:0.09749\n",
      "[26]\tvalidation_0-rmse:0.31197\tvalidation_0-mean_squared_error:0.09733\n",
      "[27]\tvalidation_0-rmse:0.31170\tvalidation_0-mean_squared_error:0.09715\n",
      "[28]\tvalidation_0-rmse:0.31138\tvalidation_0-mean_squared_error:0.09695\n",
      "[29]\tvalidation_0-rmse:0.31206\tvalidation_0-mean_squared_error:0.09738\n",
      "[0]\tvalidation_0-rmse:0.38919\tvalidation_0-mean_absolute_error:0.36700\n",
      "[1]\tvalidation_0-rmse:0.35602\tvalidation_0-mean_absolute_error:0.32348\n",
      "[2]\tvalidation_0-rmse:0.33938\tvalidation_0-mean_absolute_error:0.29799\n",
      "[3]\tvalidation_0-rmse:0.32998\tvalidation_0-mean_absolute_error:0.27905\n",
      "[4]\tvalidation_0-rmse:0.32197\tvalidation_0-mean_absolute_error:0.26263\n",
      "[5]\tvalidation_0-rmse:0.32103\tvalidation_0-mean_absolute_error:0.25399\n",
      "[6]\tvalidation_0-rmse:0.31593\tvalidation_0-mean_absolute_error:0.24465\n",
      "[7]\tvalidation_0-rmse:0.31677\tvalidation_0-mean_absolute_error:0.24091\n",
      "[8]\tvalidation_0-rmse:0.31391\tvalidation_0-mean_absolute_error:0.23530\n",
      "[9]\tvalidation_0-rmse:0.31363\tvalidation_0-mean_absolute_error:0.23194\n",
      "[10]\tvalidation_0-rmse:0.31417\tvalidation_0-mean_absolute_error:0.23221\n",
      "[11]\tvalidation_0-rmse:0.31376\tvalidation_0-mean_absolute_error:0.23064\n",
      "[12]\tvalidation_0-rmse:0.31292\tvalidation_0-mean_absolute_error:0.22937\n",
      "[13]\tvalidation_0-rmse:0.31261\tvalidation_0-mean_absolute_error:0.22790\n",
      "[14]\tvalidation_0-rmse:0.31392\tvalidation_0-mean_absolute_error:0.22751\n",
      "[15]\tvalidation_0-rmse:0.31177\tvalidation_0-mean_absolute_error:0.22615\n",
      "[16]\tvalidation_0-rmse:0.31230\tvalidation_0-mean_absolute_error:0.22563\n",
      "[17]\tvalidation_0-rmse:0.31316\tvalidation_0-mean_absolute_error:0.22594\n",
      "[18]\tvalidation_0-rmse:0.31328\tvalidation_0-mean_absolute_error:0.22561\n",
      "[19]\tvalidation_0-rmse:0.31329\tvalidation_0-mean_absolute_error:0.22509\n",
      "[20]\tvalidation_0-rmse:0.31069\tvalidation_0-mean_absolute_error:0.22210\n",
      "[21]\tvalidation_0-rmse:0.31126\tvalidation_0-mean_absolute_error:0.22246\n",
      "[22]\tvalidation_0-rmse:0.31183\tvalidation_0-mean_absolute_error:0.22324\n",
      "[23]\tvalidation_0-rmse:0.31095\tvalidation_0-mean_absolute_error:0.22177\n",
      "[24]\tvalidation_0-rmse:0.31178\tvalidation_0-mean_absolute_error:0.22219\n",
      "[25]\tvalidation_0-rmse:0.31224\tvalidation_0-mean_absolute_error:0.22297\n",
      "[26]\tvalidation_0-rmse:0.31197\tvalidation_0-mean_absolute_error:0.22268\n",
      "[27]\tvalidation_0-rmse:0.31170\tvalidation_0-mean_absolute_error:0.22237\n",
      "[28]\tvalidation_0-rmse:0.31138\tvalidation_0-mean_absolute_error:0.22183\n",
      "[29]\tvalidation_0-rmse:0.31206\tvalidation_0-mean_absolute_error:0.22206\n",
      "[30]\tvalidation_0-rmse:0.31185\tvalidation_0-mean_absolute_error:0.22202\n",
      "[31]\tvalidation_0-rmse:0.31103\tvalidation_0-mean_absolute_error:0.22152\n",
      "[32]\tvalidation_0-rmse:0.31113\tvalidation_0-mean_absolute_error:0.22141\n",
      "[33]\tvalidation_0-rmse:0.31057\tvalidation_0-mean_absolute_error:0.22098\n",
      "[34]\tvalidation_0-rmse:0.31080\tvalidation_0-mean_absolute_error:0.22156\n",
      "[35]\tvalidation_0-rmse:0.31018\tvalidation_0-mean_absolute_error:0.22078\n",
      "[36]\tvalidation_0-rmse:0.30998\tvalidation_0-mean_absolute_error:0.22066\n",
      "[37]\tvalidation_0-rmse:0.30941\tvalidation_0-mean_absolute_error:0.22048\n",
      "[38]\tvalidation_0-rmse:0.30990\tvalidation_0-mean_absolute_error:0.22129\n",
      "[39]\tvalidation_0-rmse:0.31082\tvalidation_0-mean_absolute_error:0.22206\n",
      "[40]\tvalidation_0-rmse:0.31085\tvalidation_0-mean_absolute_error:0.22157\n",
      "[41]\tvalidation_0-rmse:0.31082\tvalidation_0-mean_absolute_error:0.22130\n",
      "[42]\tvalidation_0-rmse:0.30996\tvalidation_0-mean_absolute_error:0.22126\n",
      "[43]\tvalidation_0-rmse:0.30887\tvalidation_0-mean_absolute_error:0.22071\n",
      "[44]\tvalidation_0-rmse:0.30888\tvalidation_0-mean_absolute_error:0.22066\n",
      "[45]\tvalidation_0-rmse:0.30862\tvalidation_0-mean_absolute_error:0.22036\n",
      "[46]\tvalidation_0-rmse:0.30895\tvalidation_0-mean_absolute_error:0.22038\n",
      "[47]\tvalidation_0-rmse:0.30816\tvalidation_0-mean_absolute_error:0.21996\n",
      "[48]\tvalidation_0-rmse:0.30815\tvalidation_0-mean_absolute_error:0.22013\n",
      "[49]\tvalidation_0-rmse:0.30767\tvalidation_0-mean_absolute_error:0.22004\n",
      "[50]\tvalidation_0-rmse:0.30773\tvalidation_0-mean_absolute_error:0.22023\n",
      "[51]\tvalidation_0-rmse:0.30781\tvalidation_0-mean_absolute_error:0.22019\n",
      "[52]\tvalidation_0-rmse:0.30803\tvalidation_0-mean_absolute_error:0.22043\n",
      "[53]\tvalidation_0-rmse:0.30765\tvalidation_0-mean_absolute_error:0.22030\n",
      "[54]\tvalidation_0-rmse:0.30762\tvalidation_0-mean_absolute_error:0.22045\n",
      "[55]\tvalidation_0-rmse:0.30753\tvalidation_0-mean_absolute_error:0.22042\n",
      "[56]\tvalidation_0-rmse:0.30836\tvalidation_0-mean_absolute_error:0.22124\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Median Absolute Error</th>\n",
       "      <th>Explained Variance Score</th>\n",
       "      <th>Max Error</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgb regressor with mean squared error</th>\n",
       "      <td>0.225622</td>\n",
       "      <td>0.500815</td>\n",
       "      <td>0.169094</td>\n",
       "      <td>0.503089</td>\n",
       "      <td>0.988431</td>\n",
       "      <td>0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb regressor with mean absolute error</th>\n",
       "      <td>0.221301</td>\n",
       "      <td>0.516491</td>\n",
       "      <td>0.166118</td>\n",
       "      <td>0.519198</td>\n",
       "      <td>0.971655</td>\n",
       "      <td>0.089308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest regressor with squared error</th>\n",
       "      <td>0.220909</td>\n",
       "      <td>0.548062</td>\n",
       "      <td>0.185693</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>0.844130</td>\n",
       "      <td>0.083477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Mean Absolute Error  R2 Score  \\\n",
       "xgb regressor with mean squared error                  0.225622  0.500815   \n",
       "xgb regressor with mean absolute error                 0.221301  0.516491   \n",
       "random forest regressor with squared error             0.220909  0.548062   \n",
       "\n",
       "                                            Median Absolute Error  \\\n",
       "xgb regressor with mean squared error                    0.169094   \n",
       "xgb regressor with mean absolute error                   0.166118   \n",
       "random forest regressor with squared error               0.185693   \n",
       "\n",
       "                                            Explained Variance Score  \\\n",
       "xgb regressor with mean squared error                       0.503089   \n",
       "xgb regressor with mean absolute error                      0.519198   \n",
       "random forest regressor with squared error                  0.549812   \n",
       "\n",
       "                                            Max Error  Mean Squared Error  \n",
       "xgb regressor with mean squared error        0.988431            0.092204  \n",
       "xgb regressor with mean absolute error       0.971655            0.089308  \n",
       "random forest regressor with squared error   0.844130            0.083477  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import ndtri\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "# Load dataset\n",
    "with rasterio.open('fusion.tif') as src:\n",
    "    raster = src.read()\n",
    "    bands = src.count\n",
    "    band_names = src.descriptions\n",
    "    X = raster.transpose(1, 2, 0).reshape(-1, bands)\n",
    "\n",
    "with rasterio.open('plot.tif') as src:\n",
    "    y = src.read(1).flatten()\n",
    "\n",
    "# remove nan values\n",
    "mask = ~np.isnan(X).any(axis=1)\n",
    "mask = np.logical_and(mask, ~np.isnan(y))\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "def based_train_test_split(*arrays, confidence_interval_width=0.5, confidence_niveau=0.95, seed=None):\n",
    "    z = ndtri(confidence_niveau)\n",
    "    n = z**2 / confidence_interval_width**2\n",
    "    min_sample_size = np.ceil(n).astype(int)\n",
    "\n",
    "    return train_test_split(*arrays, test_size=min_sample_size, random_state=seed)\n",
    "\n",
    "# Split the dataset into a train, eval, and test set\n",
    "X_remaining, X_test, y_remaining, y_test = based_train_test_split(\n",
    "    X, y,\n",
    "    confidence_interval_width=0.1,\n",
    "    confidence_niveau=0.95,\n",
    "    seed=1\n",
    ")\n",
    "X_train, X_eval, y_train, y_eval = based_train_test_split(\n",
    "    X_remaining, y_remaining,\n",
    "    confidence_interval_width=0.1,\n",
    "    confidence_niveau=0.95,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Eval set size: {len(X_eval)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Sample all train dataset samples where the label is 0 or 1\n",
    "mask = np.logical_or(y_train == 0, y_train == 1)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Create a list of ML models, with 5 different hyperparameters each\n",
    "models = {'xgb regressor with mean squared error': xgb.XGBRegressor(n_estimators=1000, tree_method=\"hist\", eval_metric=metrics.mean_squared_error, early_stopping_rounds=10),\n",
    "          'xgb regressor with mean absolute error': xgb.XGBRegressor(n_estimators=1000, tree_method=\"hist\", eval_metric=metrics.mean_absolute_error, early_stopping_rounds=10),\n",
    "          'random forest regressor with squared error': RandomForestRegressor(n_estimators=1000, criterion='squared_error', n_jobs=-1)}\n",
    "\n",
    "# Train all models on the train set\n",
    "for model in models:\n",
    "    try:\n",
    "        models[model].fit(X_train, y_train, eval_set=[(X_eval, y_eval)])\n",
    "    except:\n",
    "        models[model].fit(X_train, y_train)\n",
    "\n",
    "# Define the metrics to evaluate the models on\n",
    "metrics = [\n",
    "    metrics.mean_absolute_error,\n",
    "    metrics.r2_score,\n",
    "    metrics.median_absolute_error,\n",
    "    metrics.explained_variance_score,\n",
    "    metrics.max_error,\n",
    "    metrics.mean_squared_error,\n",
    "]\n",
    "\n",
    "X_eval = X_test\n",
    "y_eval = y_test\n",
    "\n",
    "# Evaluate the models on the eval set and create a new dataframe\n",
    "eval_df = pd.DataFrame({\n",
    "    metric.__name__.replace('_', ' ').title(): [metric(y_eval, model.predict(X_eval))\n",
    "                                                for model in models.values()]\n",
    "    for metric in metrics\n",
    "}, index=models.keys())\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = {model_name: [] for model_name in models.keys()}\n",
    "for model_name, model in models.items():\n",
    "    for importance in model.feature_importances_:\n",
    "        feature_importance[model_name].append(importance)\n",
    "\n",
    "feature_importance = pd.DataFrame(feature_importance, index=band_names)\n",
    "# sorted(list(zip(band_names, importances)), key=lambda x: x[1], reverse=True)\n",
    "feature_importance['mean'] = feature_importance.mean(axis=1)\n",
    "# sort by mean\n",
    "feature_importance = feature_importance.sort_values(by='mean', ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare actual and predicted mean\n",
    "np.mean(y_eval), np.mean(list(models.values())[0].predict(X_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prediction with ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = list(models.values())[0]\n",
    "\n",
    "# Load dataset\n",
    "with rasterio.open('fusion.tif') as src:\n",
    "    raster = src.read()\n",
    "    bands = src.count\n",
    "    image = raster.transpose(1, 2, 0).reshape(-1, bands)\n",
    "\n",
    "with rasterio.open('plot.tif') as src:\n",
    "    annotation = src.read(1)\n",
    "\n",
    "prediction = model.predict(image)\n",
    "prediction = prediction.reshape(annotation.shape)\n",
    "\n",
    "# mask nan values\n",
    "mask = np.isnan(annotation)\n",
    "prediction[mask] = np.nan\n",
    "prediction = np.clip(prediction, 0, 1)\n",
    "\n",
    "plt.imshow(prediction, cmap='magma')\n",
    "plt.show()\n",
    "plt.imshow(annotation, cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments (can be deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a map.\n",
    "lat, lon = 47.9357, 12.66535\n",
    "my_map = folium.Map(location=[lat, lon], zoom_start=15)\n",
    "\n",
    "# Define a method for displaying Earth Engine image tiles on a folium map.\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "    \"\"\"Adds a method for displaying Earth Engine image tiles to folium map.\"\"\"\n",
    "    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "    folium.raster_layers.TileLayer(\n",
    "        tiles=map_id_dict['tile_fetcher'].url_format,\n",
    "        attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "        name=name,\n",
    "        overlay=True,\n",
    "        control=True\n",
    "    ).add_to(self)\n",
    "\n",
    "# Add Earth Engine drawing method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer\n",
    "\n",
    "# Add the stacked composites to the map.\n",
    "dem = ee.Image('projects/leaf-type-mixture/assets/DEM')\n",
    "my_map.add_ee_layer(dem.multiply(100).toUint16().entropy(ee.Kernel.square(4)), {'min': 0, 'max': 10}, 'DEM')\n",
    "\n",
    "# Add a layer control panel to the map.\n",
    "my_map.add_child(folium.LayerControl())\n",
    "\n",
    "# Display the map.\n",
    "display(my_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Training samples: 120\n",
      "  Validation samples: 30\n",
      "  Training samples: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "  Validation samples: [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "Fold 2:\n",
      "  Training samples: 120\n",
      "  Validation samples: 30\n",
      "  Training samples: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "  Validation samples: [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Fold 3:\n",
      "  Training samples: 120\n",
      "  Validation samples: 30\n",
      "  Training samples: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "  Validation samples: [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2]\n",
      "Fold 4:\n",
      "  Training samples: 120\n",
      "  Validation samples: 30\n",
      "  Training samples: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "  Validation samples: [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2]\n",
      "Fold 5:\n",
      "  Training samples: 120\n",
      "  Validation samples: 30\n",
      "  Training samples: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "  Validation samples: [0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Number of splits: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "n_samples = len(y)\n",
    "min_val_size = 30\n",
    "n_splits = int(n_samples / min_val_size)\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)#, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Print fold information\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Validation samples: {len(X_val)}\")\n",
    "\n",
    "    print(f\"  Training samples: {y_train}\")\n",
    "    print(f\"  Validation samples: {y_val}\")\n",
    "\n",
    "# Output the number of splits\n",
    "print(\"Number of splits:\", kf.get_n_splits())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "iris = load_iris()\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200, random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(iris.data, iris.target)\n",
    "search.best_params_\n",
    "search.predict(iris.data[:1000]), iris.target[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE scores: [0.13724977 0.10070584 0.38427421 0.10045729 0.21360985]\n",
      "Gradient Boosting RMSE scores: [0.15317587 0.16812654 0.40824027 0.14468759 0.26957703]\n",
      "Best Gradient Boosting model: GradientBoostingRegressor(learning_rate=0.01, max_depth=5, n_estimators=300,\n",
      "                          random_state=42)\n",
      "Best Gradient Boosting RMSE: 0.23362877869206836\n",
      "Random Forest test RMSE: 0.037193189340702336\n",
      "Best Gradient Boosting test RMSE: 0.04167816551670047\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    scores = -cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores = np.sqrt(scores)\n",
    "    return rmse_scores\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X, y):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                               scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(X, y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_rmse = np.sqrt(-grid_search.best_score_)\n",
    "    return best_model, best_rmse\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "rf_rmse_scores = evaluate_model(rf_regressor, X_train, y_train)\n",
    "gb_rmse_scores = evaluate_model(gb_regressor, X_train, y_train)\n",
    "\n",
    "print(\"Random Forest RMSE scores:\", rf_rmse_scores)\n",
    "print(\"Gradient Boosting RMSE scores:\", gb_rmse_scores)\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "best_gb_model, best_gb_rmse = tune_hyperparameters(gb_regressor, param_grid_gb, X_train, y_train)\n",
    "\n",
    "print(\"Best Gradient Boosting model:\", best_gb_model)\n",
    "print(\"Best Gradient Boosting RMSE:\", best_gb_rmse)\n",
    "\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "rf_test_predictions = rf_regressor.predict(X_test)\n",
    "gb_test_predictions = best_gb_model.predict(X_test)\n",
    "\n",
    "rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_test_predictions))\n",
    "gb_test_rmse = np.sqrt(mean_squared_error(y_test, gb_test_predictions))\n",
    "\n",
    "print(\"Random Forest test RMSE:\", rf_test_rmse)\n",
    "print(\"Best Gradient Boosting test RMSE:\", gb_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best randomforest parameters: {'n_estimators': 100, 'max_depth': None}\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best xgboost parameters: {'n_estimators': 100, 'max_depth': 3}\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gradientboost parameters: {'n_estimators': 100, 'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier  # Make sure you have xgboost installed\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the parameter grids for each algorithm\n",
    "param_dist = {\n",
    "    'randomforest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        # other hyperparameters\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        # other hyperparameters\n",
    "    },\n",
    "    'gradientboost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        # other hyperparameters\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for each algorithm\n",
    "for model_name, param_grid in param_dist.items():\n",
    "    if model_name == 'randomforest':\n",
    "        model = RandomForestClassifier()\n",
    "    elif model_name == 'xgboost':\n",
    "        model = XGBClassifier()\n",
    "    elif model_name == 'gradientboost':\n",
    "        model = GradientBoostingClassifier()\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,  # Adjust the number of iterations as needed\n",
    "        scoring='accuracy',\n",
    "        cv=5,  # Number of cross-validation folds for hyperparameter tuning\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_val, y_val)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    # Evaluate best_model on the test set and save the results\n",
    "    \n",
    "    print(f'Best {model_name} parameters: {random_search.best_params_}')\n",
    "\n",
    "# Choose the best-performing model based on the test results\n",
    "# Train the chosen model on the combined training and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3315057  0.08022103 0.03531816]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "lasso = linear_model.Lasso()\n",
    "print(cross_val_score(lasso, X, y, cv=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
