{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducers Clustering\n",
    "\n",
    "This experiment continues the second experiment. After determining the individual performance of reducers on a varying number of composites. Now we will determine the best combination of reducers for a fix number of composites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Earth Engine API...\n"
     ]
    }
   ],
   "source": [
    "# List the bands determined by the previous experiment\n",
    "from ltm.models import bands_from_importance\n",
    "\n",
    "sentinel_bands, index_bands = bands_from_importance(\"../reports/band_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Sentinel-2 data...\n",
      "Computing data...\n",
      "GeoTIFF saved as ../data/processed/TMP_X_mega.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/processed/TMP_X_mega.tif'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltm.data import sentinel_composite\n",
    "from datetime import datetime\n",
    "\n",
    "temporal_reducers = [\n",
    "    \"median\",\n",
    "    \"mean\",\n",
    "    \"mode\",\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "    \"sampleVariance\",\n",
    "]\n",
    "\n",
    "sentinel_composite(\n",
    "    y_path_from=\"../data/processed/y.tif\",\n",
    "    X_path_to=\"../data/processed/TMP_X_mega.tif\",\n",
    "    time_window=(datetime(2017, 4, 1), datetime(2018, 4, 1)),\n",
    "    num_composites=7,  # 8, 11 is too much for GEE...\n",
    "    temporal_reducers=temporal_reducers,\n",
    "    indices=index_bands,\n",
    "    sentinel_bands=sentinel_bands,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define auto_kmeans for finding the ideal number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "\n",
    "def auto_kmeans(data, n_clusters_min=2, n_clusters_max=10, random_state=42):\n",
    "    if n_clusters_max < n_clusters_min:\n",
    "        raise ValueError(\"n_clusters_max must not be smaller than n_clusters_max\")\n",
    "    \n",
    "    # Avoid memory leak on windows\n",
    "    if platform.system() == \"Windows\":\n",
    "        os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    \n",
    "    best_score = float(\"-inf\")\n",
    "    best_kmeans = None\n",
    "    for n_clusters in range(n_clusters_min, n_clusters_max + 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
    "\n",
    "        # Suppress memory leak on windows warning\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            labels = kmeans.fit_predict(data)\n",
    "        s_score = silhouette_score(data, labels)\n",
    "\n",
    "        if s_score > best_score:\n",
    "            best_score = s_score\n",
    "            best_kmeans = kmeans\n",
    "\n",
    "    if best_kmeans is None:\n",
    "        raise ValueError(\"This error should not occur... best_kmeans should at least always be set to the first kmeans\")\n",
    "    \n",
    "    if platform.system() == \"Windows\":\n",
    "        os.environ.pop(\"OMP_NUM_THREADS\")\n",
    "    \n",
    "    return best_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 1,...,n-1 best reducer clusters for each metric and number of composites\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Create dictionary whether a scorer is a loss, a.k.a. less is better\n",
    "is_loss_dict = {\n",
    "    \"R2 Score\": False,\n",
    "    \"Mean Absolute Error\": True,\n",
    "    \"Root Mean Squared Error\": True,\n",
    "}\n",
    "\n",
    "# Create one DataFrame per Metric with reducers as index and number of composites as columns\n",
    "df = pd.read_csv(\"../reports/reducer_composites.csv\")\n",
    "dfs = {}\n",
    "for metric in is_loss_dict.keys():\n",
    "    new_df = pd.DataFrame(index=df[\"Composites\"].unique()[::-1])\n",
    "    \n",
    "    grouped = df[[\"Reducer\", \"Composites\", metric]].groupby(\"Reducer\")\n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values(\"Composites\")\n",
    "        new_df[name] = np.asarray(group[metric])\n",
    "\n",
    "    dfs[metric] = new_df.T\n",
    "\n",
    "list_of_kwargs = []\n",
    "for metric, is_loss in is_loss_dict.items():\n",
    "    df = dfs[metric]\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    for num_composites in df.columns:\n",
    "        composite_df = df[[num_composites]]\n",
    "\n",
    "        # Compute the kmeans with best silhouette score\n",
    "        kmeans = auto_kmeans(composite_df)\n",
    "        labels = kmeans.predict(composite_df)\n",
    "        n_clusters = kmeans.n_clusters\n",
    "        cluster_centers = np.array(kmeans.cluster_centers_[:, 0])\n",
    "\n",
    "        # Sort the clusters according to the metric\n",
    "        sorted_indices = np.argsort(cluster_centers)\n",
    "        if not is_loss:\n",
    "            sorted_indices = sorted_indices[::-1]\n",
    "        \n",
    "        # Create parameter configs for the 1,...,n-1 best clusters \n",
    "        for max_idx in range(1, len(sorted_indices)):\n",
    "            valid_centers = sorted_indices[:max_idx]\n",
    "            valid_reducers = [reducer for reducer, label\n",
    "                              in zip(composite_df.index, labels)\n",
    "                              if label in valid_centers]\n",
    "            \n",
    "            kwargs = {\n",
    "                \"time_window\": (datetime(2017, 4, 1), datetime(2018, 4, 1)),\n",
    "                \"num_composites\": num_composites,\n",
    "                \"temporal_reducers\": valid_reducers,\n",
    "                \"indices\": index_bands,\n",
    "                \"sentinel_bands\": sentinel_bands,\n",
    "            }\n",
    "\n",
    "            list_of_kwargs.append(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 3/97 [25:19<13:13:15, 506.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 96\u001b[0m\n\u001b[0;32m     91\u001b[0m             scores \u001b[38;5;241m=\u001b[39m {key: scores[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m scoring\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m     93\u001b[0m         callback(kwargs, scores, report_path, create_row)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mevaluate_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../reports/reducer_clustering.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/y.tif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 90\u001b[0m, in \u001b[0;36mevaluate_kwargs\u001b[1;34m(list_of_kwargs, scoring, report_path, y_path, create_X_name, create_row, callback)\u001b[0m\n\u001b[0;32m     88\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {key: np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m scoring\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_no_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_no_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {key: scores[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m scoring\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m     93\u001b[0m callback(kwargs, scores, report_path, create_row)\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Score each kwargs combination\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from ltm.data import sentinel_composite\n",
    "from ltm.features import load_raster, interpolate_X, to_float32, drop_nan_rows\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "scoring = {\n",
    "    \"R2 Score\": make_scorer(r2_score),\n",
    "    \"Mean Absolute Error\": make_scorer(mean_absolute_error),\n",
    "    \"Root Mean Squared Error\": make_scorer(mean_squared_error, squared=False)\n",
    "}\n",
    "X_counter = 1\n",
    "\n",
    "\n",
    "def create_X_name(kwargs):\n",
    "    global X_counter\n",
    "\n",
    "    X_name = f\"X_{X_counter}.tif\"\n",
    "    X_counter += 1\n",
    "\n",
    "    return X_name\n",
    "\n",
    "def create_row(kwargs):\n",
    "    # Create dictionary with strings as keys and values\n",
    "    row = {\n",
    "        \"Reducer\": \" \".join(kwargs[\"temporal_reducers\"]),\n",
    "        \"Composites\": kwargs[\"num_composites\"],\n",
    "    }\n",
    "\n",
    "    return row\n",
    "\n",
    "def check_row_exists(row, report_path):\n",
    "    if not Path(report_path).exists():\n",
    "        return False\n",
    "\n",
    "    df = pd.read_csv(report_path)\n",
    "\n",
    "    for _, existing_row in df.iterrows():\n",
    "        existing_row = dict(existing_row)\n",
    "        if all(k in existing_row.keys() and v == existing_row[k]\n",
    "               for k, v in row.items()):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def create_X(y_path, kwargs, create_X_name=create_X_name):\n",
    "    # Use existing X_{temporal_reducer}_{num_composites}.tif\n",
    "    Xs = []\n",
    "    for reducer in kwargs[\"temporal_reducers\"]:\n",
    "        X_path_to = f\"../data/processed/reducer_composites/X_{reducer}_{kwargs['num_composites']}.tif\"\n",
    "        X = load_raster(X_path_to)\n",
    "        Xs.append(X)\n",
    "    \n",
    "    # Fill missing values by linear (circular) interpolation\n",
    "    X = pd.concat(Xs, axis=1)\n",
    "    interpolated_X = interpolate_X(X)\n",
    "    interpolated_X = to_float32(interpolated_X)\n",
    "    \n",
    "    return interpolated_X\n",
    "\n",
    "def callback(kwargs, scores, report_path, create_row=create_row):\n",
    "    row = create_row(kwargs)\n",
    "    row.update(scores)\n",
    "\n",
    "    if not Path(report_path).exists():\n",
    "        df = pd.DataFrame([row])\n",
    "    else:\n",
    "        df = pd.read_csv(report_path)\n",
    "        df.loc[len(df)] = row\n",
    "\n",
    "    df.to_csv(report_path, index=False)\n",
    "\n",
    "def evaluate_kwargs(list_of_kwargs, scoring, report_path, y_path, create_X_name=create_X_name, create_row=create_row, callback=callback):\n",
    "    y = load_raster(y_path)\n",
    "    model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "    for kwargs in tqdm(list_of_kwargs):\n",
    "        if check_row_exists(create_row(kwargs), report_path):\n",
    "            continue\n",
    "\n",
    "        X = create_X(y_path, kwargs, create_X_name)\n",
    "        X_no_nan, y_no_nan = drop_nan_rows(X, y)\n",
    "\n",
    "        if len(X_no_nan) < 5:\n",
    "            scores = {key: np.nan for key in scoring.keys()}\n",
    "        else:\n",
    "            scores = cross_validate(model, X_no_nan, y_no_nan, scoring=scoring, n_jobs=-1)\n",
    "            scores = {key: scores[f\"test_{key}\"].mean() for key in scoring.keys()}\n",
    "        \n",
    "        callback(kwargs, scores, report_path, create_row)\n",
    "\n",
    "\n",
    "evaluate_kwargs(list_of_kwargs, scoring, \"../reports/reducer_clustering.csv\", \"../data/processed/y.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 2, 22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_bands = len(list_of_kwargs[3][\"indices\"]) + len(list_of_kwargs[3][\"sentinel_bands\"])\n",
    "num_composites = list_of_kwargs[3][\"num_composites\"]\n",
    "num_reducers = len(list_of_kwargs[3][\"temporal_reducers\"])\n",
    "\n",
    "num_bands, num_composites, num_reducers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
