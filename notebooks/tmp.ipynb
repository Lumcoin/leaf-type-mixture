{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traunstein\n",
    "\n",
    "If a tree has multiple stems, each stem is treates as a separate entity. Only alive stems are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# To crs 4326\n",
    "gdf = gpd.read_file(r\"C:\\Users\\hofin\\Downloads\\Baumkoordinaten_mh\\Baumkoordinaten_mh.shp\")\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "evergreen = {\n",
    "    \"Abies alba\": True,\n",
    "    \"Acer campestre\": False,\n",
    "    \"Acer platanoides\": False,\n",
    "    \"Acer pseudoplatanus\": False,\n",
    "    \"Aesculus hippocastanum\": False,\n",
    "    \"Alnus glutinose\": False,\n",
    "    \"Betula \": False,\n",
    "    \"Carpinus betulus\": False,\n",
    "    \"Fagus sylvatica\": False,\n",
    "    \"Fraxinus excelsior\": False,\n",
    "    \"Juglans regia\": False,\n",
    "    \"Larix decidua\": False,\n",
    "    \"Picea abies\": True,\n",
    "    \"Pinus sylvestris\": True,\n",
    "    \"Populus \": False,\n",
    "    \"Populus tremula\": False,\n",
    "    \"Prunus avium\": False,\n",
    "    \"Pseudotsuga menziesii\": True,\n",
    "    \"Quercus \": False,\n",
    "    \"Quercus rubra\": False,\n",
    "    \"Salix \": False,\n",
    "    \"Sorbus aria\": False,\n",
    "    \"Sorbus aucuparia\": False,\n",
    "    \"Sorbus torminalis\": False,\n",
    "    \"Thuja plicata\": True,\n",
    "    \"Tilia \": False,\n",
    "    \"Ulmus glabra\": False,\n",
    "    \"Unidentified broadleaf\": False,\n",
    "    \"Unidentified conifer\": True,\n",
    "}\n",
    "gdf[\"Latin\"] = gdf[[\"Latin\", \"Mnemonic\"]].agg(' '.join, axis=1)\n",
    "gdf[\"Evergreen\"] = gdf[\"Latin\"].map(evergreen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = gdf[[\"Evergreen\", \"Date\", \"DBH\"]]\n",
    "plot.columns = [\"conifer\", \"date\", \"dbh\"]\n",
    "plot[\"conifer\"] = plot[\"conifer\"].astype(int)\n",
    "plot[\"dbh\"] = plot[\"dbh\"] / 1000\n",
    "plot[\"latitude\"] = gdf[\"geometry\"].y\n",
    "plot[\"longitude\"] = gdf[\"geometry\"].x\n",
    "plot.to_csv(\"plot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naturwaldreservate\n",
    "\n",
    "TODO: Resample the dataset from uniform labels [0, 1], or use SMOTE or other imbalance techniques. See https://imbalanced-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "is_conifer = {\n",
    "    'Ah': False,\n",
    "    'BAh': False,\n",
    "    'Eibe': True,\n",
    "    'Bu': False,\n",
    "    'Ei': False,\n",
    "    'SAh': False,\n",
    "    'ScDo': False,\n",
    "    'FAh': False,\n",
    "    'Li': False,\n",
    "    'Elsb': False,\n",
    "    'Fi': True,\n",
    "    'HBu': False,\n",
    "    'Ta': True,\n",
    "    'BUl': False,\n",
    "    'As': False,\n",
    "    'Es': False,\n",
    "    'WLi': False,\n",
    "    'FUl': False,\n",
    "    'SErl': False,\n",
    "    'Ul': False,\n",
    "    'Mehlb': False,\n",
    "    'Erl': False,\n",
    "    'Lae': True,\n",
    "    'Bi': False,\n",
    "    'SHol': False,\n",
    "    'VKir': False,\n",
    "    'Has': False,\n",
    "    'SLi': False,\n",
    "    'WDo': False,\n",
    "    'Kie': True,\n",
    "    'Stro': True,\n",
    "    'TrKir': False,\n",
    "    'SPa': False,\n",
    "    'Hartr': False,\n",
    "    'Pfaffh': False,\n",
    "    'WErl': False,\n",
    "    'Vobe': False,\n",
    "    'Hol': False,\n",
    "    'StEi': False,\n",
    "    'Dgl': True,\n",
    "    'WDom': False,\n",
    "    'Kir': False,\n",
    "    'GPa': False,\n",
    "    'Birne': False,\n",
    "    'Wei': False,\n",
    "    'Spi': False,\n",
    "    'Zir': True,\n",
    "    'KreuzD': False,\n",
    "    'WObst': False,\n",
    "    'SWei': False,\n",
    "    'JLä': True,\n",
    "    'ei': False,\n",
    "    'TrEi': False,\n",
    "}\n",
    "\n",
    "df = pd.read_excel(\"../data/raw/NWR.xlsx\")\n",
    "\n",
    "rep = df[\"NWR\"]\n",
    "\n",
    "year = df[\"aufnahmeja\"]#.map(lambda x: 2020 if not isinstance(x, str) else int(float(x.replace(\",\", \".\"))))\n",
    "dbh = df[\"DM\"] / 100\n",
    "conifer = df[\"BA\"].map(is_conifer)\n",
    "\n",
    "coords = gpd.points_from_xy(df[\"CENTROID_X\"], df[\"CENTROID_Y\"], crs=\"EPSG:32632\").to_crs(\"EPSG:4326\")\n",
    "latitude = coords.y\n",
    "longitude = coords.x\n",
    "\n",
    "cleaned_df = pd.DataFrame({\n",
    "    \"rep\": rep,\n",
    "    \"conifer\": conifer.astype(int),\n",
    "    \"date\": year.map(lambda x: f\"{x}-01-01\"),\n",
    "    \"dbh\": dbh,\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rep_id in cleaned_df[\"rep\"].unique():\n",
    "    rep_df = cleaned_df[cleaned_df[\"rep\"] == rep_id]\n",
    "    rep_df = rep_df.drop(\"rep\", axis=1)\n",
    "\n",
    "    rep_df.to_csv(f\"../data/interim/{rep_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truths\n",
    "from ltm.data import compute_target\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "targets_folder = \"../data/processed/generalization/targets/\"\n",
    "Path(targets_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for rep_id in tqdm(cleaned_df[\"rep\"].unique()):\n",
    "    target_path = targets_folder + f\"{rep_id}.tif\"\n",
    "    if not Path(target_path).exists():\n",
    "        plot = pd.read_csv(f\"../data/interim/{rep_id}.csv\")\n",
    "        if not any(plot[\"latitude\"].isna()):  # for rep_id 132\n",
    "            compute_target(target_path, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "total = 0\n",
    "counts = {}\n",
    "for rep_id in cleaned_df[\"rep\"].unique():\n",
    "    if rep_id == 132:\n",
    "        continue\n",
    "    with rasterio.open(targets_folder + f\"{rep_id}.tif\") as src:\n",
    "        count = np.sum(~np.isnan(src.read(1)))\n",
    "        total += count\n",
    "        counts[rep_id] = count\n",
    "\n",
    "print(\"Gesamtfläche [Hektar]:\", total / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from ltm.models import create_data, sentinel_composite\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "target_folder = \"../data/processed/generalization/targets/\"\n",
    "\n",
    "ids = glob(\"../data/processed/generalization/targets/*.tif\")\n",
    "ids = sorted(int(Path(target_id).stem) for target_id in ids)\n",
    "\n",
    "for rep_id in tqdm(ids):\n",
    "    # Read CSV and get all years\n",
    "    plot = pd.read_csv(f\"../data/interim/{rep_id}.csv\")\n",
    "    date = plot[\"date\"].unique()[0]\n",
    "    year = datetime.strptime(date, \"%Y-%m-%d\").year\n",
    "\n",
    "    if year < 2018:\n",
    "        print(f\"Skipping {rep_id} because year is {year} (before 2018)\")\n",
    "        continue\n",
    "\n",
    "    data_folder = str(Path(target_folder) / str(rep_id))\n",
    "    target_path = str(Path(target_folder) / f\"{rep_id}.tif\")\n",
    "\n",
    "    # Create folder\n",
    "    Path(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "    create_data(year, target_path, data_folder)\n",
    "    # data_path = str(Path(data_folder) / \"data.tif\")\n",
    "    # time_window = (datetime(year, 1, 1), datetime(year + 1, 1, 1))\n",
    "\n",
    "    # if not Path(data_path).exists():\n",
    "    #     sentinel_composite(target_path, data_path, num_composites=6, sentinel_bands=[\"TCI_R\", \"TCI_G\", \"TCI_B\"], indices=[\"NDVI\"], time_window=time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltm.features import load_raster\n",
    "\n",
    "# data_paths = glob(\"../data/processed/generalization/targets/*/data.tif\")\n",
    "data_paths = glob(\"../data/processed/generalization/targets/*/*/data.tif\")\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "for data_path in tqdm(data_paths):\n",
    "    # rep_id = Path(data_path).parent.name\n",
    "    # target_folder = Path(data_path).parent.parent\n",
    "    rep_id = Path(data_path).parent.parent.name\n",
    "    target_folder = Path(data_path).parent.parent.parent\n",
    "    target_path = str(target_folder / f\"{rep_id}.tif\")\n",
    "\n",
    "    target = load_raster(target_path)\n",
    "    conifer_proportion = target.mean()\n",
    "    if True:# 0.2 <= conifer_proportion <= 0.8:\n",
    "        data = load_raster(data_path)\n",
    "\n",
    "        mask = target.notna()\n",
    "        data = data[mask]\n",
    "        target = target[mask]\n",
    "\n",
    "        data_list.append(data)\n",
    "        target_list.append(target)\n",
    "\n",
    "test_data = pd.concat(data_list).reset_index(drop=True)\n",
    "test_target = pd.concat(target_list).reset_index(drop=True)\n",
    "\n",
    "from ltm.features import interpolate_data\n",
    "\n",
    "test_data = interpolate_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Create data for multiple years\n",
    "target_path = \"../data/processed/target.tif\"\n",
    "data_folder = \"../data/processed/generalization/targets/data/\"\n",
    "Path(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "refit_years = [2018, 2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "for refit_year in tqdm(refit_years, desc=\"Years\"):\n",
    "    create_data(refit_year, target_path, data_folder)\n",
    "    # data_path = data_folder + f\"{refit_year}/data.tif\"\n",
    "    # if not Path(data_path).exists():\n",
    "    #     Path(data_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    #     sentinel_composite(target_path, data_path, num_composites=6, sentinel_bands=[\"TCI_R\", \"TCI_G\", \"TCI_B\"], indices=[\"NDVI\"], time_window=(datetime(refit_year, 1, 1), datetime(refit_year + 1, 1, 1)))\n",
    "\n",
    "# Concatenate all data into one dataframe\n",
    "print(\"Combining data...\")\n",
    "total_data = pd.DataFrame()\n",
    "for refit_year in tqdm(refit_years, desc=\"Years\"):\n",
    "    stem = Path(data_folder).stem\n",
    "    data_path = Path(data_folder) / f\"{refit_year}/data.tif\"\n",
    "    data = load_raster(str(data_path))\n",
    "    total_data = pd.concat([total_data, data])\n",
    "\n",
    "total_data = interpolate_data(total_data)\n",
    "\n",
    "# Create target data\n",
    "target = load_raster(target_path)\n",
    "total_target = pd.concat([target] * len(refit_years))\n",
    "\n",
    "total_data = total_data.reset_index(drop=True)\n",
    "total_target = total_target.reset_index(drop=True)\n",
    "\n",
    "# Drop rows with NaN label\n",
    "heights = load_raster(\"../data/processed/DEM_median.tif\")\n",
    "heights = pd.concat([heights] * len(refit_years))\n",
    "mask = total_target.notna() #& (heights > 20)\n",
    "data, target = total_data[mask], total_target[mask]\n",
    "\n",
    "refitted_path = f\"../models/refitted.pkl\"\n",
    "\n",
    "with open(refitted_path, \"rb\") as f:\n",
    "    refitted = dill.load(f)\n",
    "\n",
    "refitted = clone(refitted)\n",
    "refitted.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute dummy RMSE\n",
    "\n",
    "# Mean RMSE on old data: 0.45560590764009135 with a mean conifer proportion of 0.602...\n",
    "# Mean RMSE on new data: 0.32092856066562325 with a mean conifer proportion of 0.148...\n",
    "# RMSE is way better without pure pixels\n",
    "# Check whether the data is read incorrectly\n",
    "# Try target computed from evergreen proportion (with larix) instead of conifer proportion\n",
    "# Try with only endmember pixels\n",
    "# Try without endmember pixels\n",
    "# Try selecting areas with 0.5 +- 0.1 conifer proportion and < 50 % endmember pixels\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "pkls = glob(\"../models/*.pkl\")\n",
    "studies = glob(\"../models/*_study.pkl\")\n",
    "\n",
    "models = [pkl for pkl in pkls if pkl not in studies]\n",
    "\n",
    "for model_path in models:\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = dill.load(f)\n",
    "    \n",
    "    try:\n",
    "        model.fit(data, target)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(f\"Score: {root_mean_squared_error(test_target, model.predict(test_data))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = ~(test_target == test_target.astype(int))\n",
    "tmp_target = test_target#[mask]\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "root_mean_squared_error(tmp_target, refitted.predict(test_data))#[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE per rep\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sizes = []\n",
    "rmses = []\n",
    "ids = []\n",
    "for single_data, single_target, data_path in zip(data_list, target_list, data_paths):\n",
    "    pred = refitted.predict(single_data)\n",
    "    rmse = root_mean_squared_error(single_target, pred)\n",
    "\n",
    "    # plt.scatter(target, pred, alpha=0.5)\n",
    "    # plt.show()\n",
    "\n",
    "    sizes.append(len(single_data))\n",
    "    rmses.append(rmse)\n",
    "    ids.append(Path(data_path).parent.parent.name)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"size\": sizes,\n",
    "    \"rmse\": rmses,\n",
    "}, index=ids)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refitted = clone(refitted)\n",
    "refitted.fit(test_data, test_target)\n",
    "\n",
    "prediction = refitted.predict(data)\n",
    "\n",
    "plt.scatter(target, prediction, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(target, prediction)\n",
    "\n",
    "# -> performance improves if filtered by height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_mean_squared_error(target, prediction)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(refitted, data, target, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_mean_squared_error(target, prediction)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(refitted, test_data, test_target, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception\n",
    "\n",
    "import dill\n",
    "from ltm.features import load_raster\n",
    "\n",
    "def predict_on(data_path, prediction_path, model):\n",
    "    data = load_raster(data_path)\n",
    "    prediction = model.predict(data)\n",
    "\n",
    "    # Read profile and raster params\n",
    "    with rasterio.open(prediction_path) as src:\n",
    "        profile = src.profile\n",
    "        shape = src.read().shape\n",
    "        nan_mask = np.isnan(src.read())\n",
    "\n",
    "    # Write prediction to target raster\n",
    "    with rasterio.open(prediction_path, \"w\", **profile) as dst:\n",
    "        reshaped = prediction.reshape(shape)\n",
    "        reshaped[nan_mask] = np.nan\n",
    "        dst.write(reshaped)\n",
    "        dst.descriptions = (\"Conifer Proportion\",)\n",
    "\n",
    "refitted_path = f\"../models/refitted.pkl\"\n",
    "\n",
    "with open(refitted_path, \"rb\") as f:\n",
    "    refitted = dill.load(f)\n",
    "\n",
    "data_path = f\"data/2020/data.tif\"\n",
    "prediction_path = f\"prediction.tif\"\n",
    "\n",
    "predict_on(data_path, prediction_path, refitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to two rasters\n",
    "import rasterio\n",
    "from ltm.data import compute_label\n",
    "from rasterio.plot import show\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot = pd.read_csv(\"../data/interim/5.csv\")\n",
    "\n",
    "# compute_label(target_path=\"plot.tif\", plot=plot)\n",
    "\n",
    "cmap = mpl.cm.viridis\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "mappable = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "with rasterio.open(\"plot.tif\") as src:\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.colorbar(mappable, ax=ax, label=\"Conifer Proportion\", shrink=0.8)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    show(src, ax=ax, cmap=cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for the target area\n",
    "from ltm.models import create_data\n",
    "\n",
    "create_data(2020, \"plot.tif\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on new data of the study area\n",
    "import dill\n",
    "from ltm.features import load_raster\n",
    "import numpy as np\n",
    "\n",
    "def predict_on(data_path, prediction_path, model):\n",
    "    data = load_raster(data_path)\n",
    "    prediction = model.predict(data)\n",
    "\n",
    "    # Read profile and raster params\n",
    "    with rasterio.open(prediction_path) as src:\n",
    "        profile = src.profile\n",
    "        shape = src.read().shape\n",
    "        nan_mask = np.isnan(src.read())\n",
    "\n",
    "    # Write prediction to target raster\n",
    "    with rasterio.open(prediction_path, \"w\", **profile) as dst:\n",
    "        reshaped = prediction.reshape(shape)\n",
    "        reshaped[nan_mask] = np.nan\n",
    "        dst.write(reshaped)\n",
    "        dst.descriptions = (\"Conifer Proportion\",)\n",
    "\n",
    "refitted_path = f\"../models/refitted.pkl\"\n",
    "\n",
    "with open(refitted_path, \"rb\") as f:\n",
    "    refitted = dill.load(f)\n",
    "\n",
    "data_path = f\"data/2020/data.tif\"\n",
    "prediction_path = f\"prediction.tif\"\n",
    "\n",
    "predict_on(data_path, prediction_path, refitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(\"prediction.tif\") as src:\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.colorbar(mappable, ax=ax, label=\"Conifer Proportion\", shrink=0.8)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    show(src, ax=ax, cmap=cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = load_raster(\"plot.tif\")\n",
    "prediction = load_raster(\"prediction.tif\")\n",
    "\n",
    "mask = ~np.isnan(target)\n",
    "target = target[mask]\n",
    "prediction = prediction[mask]\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "root_mean_squared_error(target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert SVGs to PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from cairosvg import svg2png\n",
    "\n",
    "svgs = glob(\"../reports/figures/*/*.svg\")\n",
    "pngs = [str(Path(svg).with_suffix(\".png\")) for svg in svgs]\n",
    "\n",
    "for svg, png in zip(svgs, pngs):\n",
    "    svg2png(url=svg, write_to=png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from ltm.features import load_raster\n",
    "\n",
    "regressors = glob(\"../models/*.pkl\")\n",
    "studies = glob(\"../models/*_study.pkl\")\n",
    "\n",
    "regressors = [regressor for regressor in regressors if regressor not in studies and \"cache\" not in regressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = load_raster(\"../data/processed/target.tif\")\n",
    "data = load_raster(\"../data/processed/generalization/data/2020/data.tif\")\n",
    "\n",
    "mask = target.notna()\n",
    "target = target[mask]\n",
    "data = data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def resample(data, target, sampler=None, num_samples=None):\n",
    "    df = data.copy()\n",
    "    df[\"target\"] = target\n",
    "\n",
    "    if sampler is None:\n",
    "        sampler = lambda: np.random.uniform(0, 1)\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = len(df)\n",
    "    \n",
    "    new_rows = []\n",
    "    for i in range(num_samples):\n",
    "        sample = sampler()\n",
    "        \n",
    "        # find closest row\n",
    "        row = df.iloc[(target - sample).abs().argmin()]\n",
    "        new_rows.append(row)\n",
    "    \n",
    "    res_data = pd.DataFrame(new_rows, columns=df.columns)\n",
    "\n",
    "    data = res_data.drop(\"target\", axis=1)\n",
    "    target = res_data[\"target\"]\n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin(series, num_bins: int, lower_bound: float=None, upper_bound: float=None):\n",
    "    if lower_bound is None:\n",
    "        lower_bound = series.min()\n",
    "    if upper_bound is None:\n",
    "        upper_bound = series.max()\n",
    "    \n",
    "    if num_bins < 1:\n",
    "        raise ValueError\n",
    "    if upper_bound < lower_bound:\n",
    "        raise ValueError\n",
    "\n",
    "    bin_size = (upper_bound - lower_bound) / num_bins\n",
    "    thresholds = [float(\"-inf\")] + [lower_bound + i*bin_size for i in range(1, num_bins)] + [float(\"inf\")]\n",
    "\n",
    "    bins = []\n",
    "    for lower, upper in zip(thresholds[:-1], thresholds[1:]):\n",
    "        mask = (series >= lower) & (series < upper)\n",
    "        bin = series[mask]\n",
    "        bins.append(bin)\n",
    "\n",
    "    return bins\n",
    "\n",
    "def find_largest_bin(series, min_size=6):\n",
    "    largest_binsize = 1\n",
    "    while True:\n",
    "        bins = bin(series, largest_binsize)\n",
    "        smallest_bin = min(len(bin) for bin in bins)\n",
    "\n",
    "        if smallest_bin < min_size:\n",
    "            largest_binsize -= 1\n",
    "            break\n",
    "\n",
    "        largest_binsize += 1\n",
    "    \n",
    "    return largest_binsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "bin_size = find_largest_bin(test_target, np.ceil(6 * 5/4))\n",
    "\n",
    "X = test_data.dropna(axis=1)\n",
    "# y = target\n",
    "y = np.round(test_target * bin_size).astype(int)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/5)#, stratify=y)\n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "# X_resampled, y_resampled = X_train, y_train\n",
    "\n",
    "y_resampled = y_resampled / bin_size\n",
    "# # y_test = y_test / bin_size\n",
    "\n",
    "# model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "# model.fit(X_resampled, y_resampled)\n",
    "# # y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X_train = res_data.drop(\"target\", axis=1)\n",
    "# y_train = res_data[\"target\"]\n",
    "\n",
    "# dummy = DummyRegressor()\n",
    "# dummy = deepcopy(refitted)\n",
    "# dummy.fit(X_train, y_train)\n",
    "# dummy\n",
    "with open(\"../models/SVR.pkl\", \"rb\") as f:\n",
    "    dummy = dill.load(f)\n",
    "\n",
    "\n",
    "# mask = total_target.notna()\n",
    "# data, target = total_data[mask], total_target[mask]\n",
    "\n",
    "\n",
    "dummy.fit(data, target)\n",
    "\n",
    "dummy_pred = dummy.predict(test_data)\n",
    "\n",
    "mask = (test_target != 0) & (test_target != 1)\n",
    "test_target = test_target[mask]\n",
    "dummy_pred = dummy_pred[mask]\n",
    "\n",
    "plt.scatter(test_target, dummy_pred, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "# model = DummyRegressor()\n",
    "# model = deepcopy(refitted)\n",
    "\n",
    "# cv_data = data#test_data#\n",
    "# cv_target = target#test_target#\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(cv_data, cv_target, test_size=1/5, shuffle=True, random_state=42)\n",
    "\n",
    "X_train, y_train = data, target\n",
    "X_test, y_test = test_data, test_target\n",
    "# X_train, y_train = test_data, test_target\n",
    "# X_test, y_test = data, target\n",
    "X_test, y_test = resample(X_test, y_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "# cross_val_score(model, cv_data, cv_target, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Traunstein w/o resampling: 0.287823994888369\n",
    "# Traunstein w/ resampling: 0.3163959917390755\n",
    "# Traunstein generalization w/ resampling: 0.2881937400211223\n",
    "# Traunstein dummy generalization w/ resampling: 0.3057842981793853\n",
    "\n",
    "# NWR w/o resampling: 0.17353728474049634\n",
    "# NWR w resampling: 0.3411365410789887\n",
    "# NWR generalization w resampling: 0.3271471925202301  <- this should decrease\n",
    "# NWR dummy generalization w resampling: 0.3075913736494596\n",
    "# NWR refit generalization w resampling: 0.3461570392864428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'phi_params' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m data\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## conduct smogn\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m housing_smogn \u001b[38;5;241m=\u001b[39m \u001b[43msmogn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmoter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrel_coef\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrel_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextremes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\lib\\site-packages\\smogn\\smoter.py:158\u001b[0m, in \u001b[0;36msmoter\u001b[1;34m(data, y, k, pert, samp_method, under_samp, drop_na_col, drop_na_row, replace, rel_thres, rel_method, rel_xtrm_type, rel_coef, rel_ctrl_pts_rg)\u001b[0m\n\u001b[0;32m    153\u001b[0m y_sort \u001b[38;5;241m=\u001b[39m y_sort[d \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m## -------------------------------- phi --------------------------------- ##\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m## calculate parameters for phi relevance function\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m## (see 'phi_ctrl_pts()' function for details)\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m phi_params \u001b[38;5;241m=\u001b[39m \u001b[43mphi_ctrl_pts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_sort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m## y (ascending)\u001b[39;49;00m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m## defaults \"auto\" \u001b[39;49;00m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxtrm_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_xtrm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## defaults \"both\"\u001b[39;49;00m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_coef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m## defaults 1.5\u001b[39;49;00m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctrl_pts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_ctrl_pts_rg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## user spec\u001b[39;49;00m\n\u001b[0;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m## calculate the phi relevance function\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m## (see 'phi()' function for details)\u001b[39;00m\n\u001b[0;32m    169\u001b[0m y_phi \u001b[38;5;241m=\u001b[39m phi(\n\u001b[0;32m    170\u001b[0m     \n\u001b[0;32m    171\u001b[0m     y \u001b[38;5;241m=\u001b[39m y_sort,                \u001b[38;5;66;03m## y (ascending)\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     ctrl_pts \u001b[38;5;241m=\u001b[39m phi_params      \u001b[38;5;66;03m## from 'phi_ctrl_pts()'\u001b[39;00m\n\u001b[0;32m    173\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hofin\\mambaforge\\envs\\ltm\\lib\\site-packages\\smogn\\phi_ctrl_pts.py:77\u001b[0m, in \u001b[0;36mphi_ctrl_pts\u001b[1;34m(y, method, xtrm_type, coef, ctrl_pts)\u001b[0m\n\u001b[0;32m     74\u001b[0m     phi_params \u001b[38;5;241m=\u001b[39m phi_range(ctrl_pts)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m## return phi relevance parameters dictionary\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mphi_params\u001b[49m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'phi_params' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from ltm.features import load_raster\n",
    "\n",
    "import smogn\n",
    "\n",
    "data = load_raster(\"../data/processed/ground_truth/data_2A.tif\")\n",
    "target = load_raster(\"../data/processed/target.tif\")\n",
    "\n",
    "mask = target.notna() & (target != 0) & (target != 1)\n",
    "data = data[mask]\n",
    "target = target[mask]\n",
    "\n",
    "data[\"target\"] = target\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "## conduct smogn\n",
    "housing_smogn = smogn.smoter(\n",
    "    data = data, \n",
    "    y = \"target\",\n",
    "    rel_coef = 0.01,\n",
    "    rel_method = \"extremes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mask = (target == 0) | (target == 1)\n",
    "target_clf = target[mask]\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "model.fit(data[mask], target_clf)\n",
    "\n",
    "mask = (test_target == 0) | (test_target == 1)\n",
    "test_target_clf = test_target[mask]\n",
    "\n",
    "pred = model.predict(test_data[mask])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test_target_clf, pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_target_clf, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(target, dummy_pred)  # 0.30224987799459396 with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = res_data.drop(\"target\", axis=1)\n",
    "y_test = res_data[\"target\"]\n",
    "y_pred = refitted.predict(X_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idcs = np.random.choice(len(X_resampled), len(test_data), replace=False)\n",
    "X_train, y_train = X_resampled.iloc[train_idcs], y_resampled.iloc[train_idcs]\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(data.dropna(axis=1))\n",
    "plt.scatter(target, y_pred, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "predictions = cross_val_predict(model, train_data, train_target, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = res_data.drop(\"target\", axis=1).dropna(axis=1)\n",
    "y_test = res_data[\"target\"]\n",
    "\n",
    "predictions = model.predict(X_test.dropna(axis=1))\n",
    "\n",
    "plt.scatter(y_test, predictions, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "model_paths = glob(\"../tmp/models/*.pkl\")\n",
    "model_paths = [path for path in model_paths if \"cache\" not in path and \"study\" not in path]\n",
    "\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot of the conifer proportion\n",
    "import seaborn as sns\n",
    "\n",
    "wo_endmembers = target[(target < 1) & (target > 0)]\n",
    "ax = sns.kdeplot(wo_endmembers, bw_adjust=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "for model_path in model_paths:\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = dill.load(f)\n",
    "\n",
    "    try:\n",
    "        print(model_path, root_mean_squared_error(train_target, model.predict(new_train_data)))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Generate synthetic imbalanced regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, noise=0.1, random_state=42)\n",
    "\n",
    "# Introduce imbalance by duplicating some target values\n",
    "y[900:] = y.max()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Random Over-Sampling to address imbalance\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a linear regression model on the resampled data\n",
    "model = LinearRegression()\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluate the model on the original test set\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"Model R-squared score on original test set:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(regressors[0], \"rb\") as f:\n",
    "    model = dill.load(f)\n",
    "\n",
    "prediction = model.predict(data)\n",
    "print(root_mean_squared_error(target, prediction))\n",
    "\n",
    "plt.scatter(target, prediction, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(regressors[1], \"rb\") as f:\n",
    "    model = dill.load(f)\n",
    "\n",
    "prediction = model.predict(data)\n",
    "print(root_mean_squared_error(target, prediction))\n",
    "\n",
    "plt.scatter(target, prediction, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(regressors[2], \"rb\") as f:\n",
    "    model = dill.load(f)\n",
    "\n",
    "prediction = model.predict(data)\n",
    "print(root_mean_squared_error(target, prediction))\n",
    "\n",
    "plt.scatter(target, prediction, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(regressors[3], \"rb\") as f:\n",
    "    model = dill.load(f)\n",
    "\n",
    "prediction = model.predict(data)\n",
    "print(root_mean_squared_error(target, prediction))\n",
    "\n",
    "plt.scatter(target, prediction, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(regressors[4], \"rb\") as f:\n",
    "    model = dill.load(f)\n",
    "\n",
    "prediction = model.predict(data)\n",
    "print(root_mean_squared_error(target, prediction))\n",
    "\n",
    "plt.scatter(target, prediction, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(regressors[5], \"rb\") as f:\n",
    "    model = dill.load(f)\n",
    "\n",
    "prediction = model.predict(data)\n",
    "print(root_mean_squared_error(target, prediction))\n",
    "\n",
    "plt.scatter(target, prediction, alpha=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
