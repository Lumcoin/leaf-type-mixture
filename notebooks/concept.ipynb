{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Earth Engine API...\n",
      "Preparing PALSAR data...\n",
      "Computing data...\n",
      "GeoTIFF saved as ../data/processed/y_palsar.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/processed/y_palsar.tif'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltm.data import palsar_raster\n",
    "\n",
    "y_path = \"../data/processed/y.tif\"\n",
    "palsar_raster(y_path, \"../data/processed/y_palsar.tif\", timestamp=\"2016-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltm.features import load_raster, interpolate_X, drop_nan_rows, save_raster, get_similarity_matrix, show_similarity_matrix, show_dendrogram, dendrogram_dim_red\n",
    "\n",
    "X_path = \"../data/processed/X.tif\"\n",
    "y_path = \"../data/processed/y.tif\"\n",
    "\n",
    "X = load_raster(X_path)\n",
    "y = load_raster(y_path)\n",
    "\n",
    "X = interpolate_X(X)\n",
    "\n",
    "# similarity_matrix = get_similarity_matrix(X)\n",
    "# show_similarity_matrix(similarity_matrix)\n",
    "# show_dendrogram(similarity_matrix)\n",
    "# dendrogram_dim_red(X, similarity_matrix, threshold=0.2)\n",
    "\n",
    "# save_raster(X, X_path, \"../data/processed/interpolated_X.tif\")\n",
    "\n",
    "X, y = drop_nan_rows(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination\n",
    "\n",
    "Unfortunately pretty slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Perform Recursive Feature Elimination\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.metrics import make_scorer, median_absolute_error\n",
    "import numpy as np\n",
    "from seaborn import pairplot\n",
    "\n",
    "def sample(X, y, n):\n",
    "    indices = np.random.choice(len(X), size=n, replace=False)\n",
    "\n",
    "    mask = np.zeros(len(X), dtype=bool)\n",
    "    mask[indices] = True\n",
    "\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# X, y = sample(X, y, 100)\n",
    "\n",
    "seed = 0\n",
    "\n",
    "rfe = RFE(\n",
    "    RandomForestRegressor(random_state=seed),\n",
    "    n_features_to_select=5,\n",
    "    verbose=1,\n",
    ")\n",
    "rfe.fit(X, y)\n",
    "rfe_columns = rfe.get_feature_names_out()\n",
    "rfe_X = X[rfe_columns]\n",
    "\n",
    "print(rfe_columns)\n",
    "pairplot(rfe_X)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    RandomForestRegressor(random_state=seed),\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "rfecv_columns = rfecv.get_feature_names_out()\n",
    "rfecv_X = X[rfecv_columns]\n",
    "\n",
    "print(rfecv_columns)\n",
    "pairplot(rfecv_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rfecv.cv_results_[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legit Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composite images from plot and Sentinel-2 data\n",
    "from ltm.data import compute_label, sentinel_composite\n",
    "import pandas as pd\n",
    "\n",
    "plot = pd.read_csv(\"../data/processed/plot.csv\")\n",
    "\n",
    "y_path = \"../data/processed/y.tif\"\n",
    "y_area_path = \"../data/processed/y_area.tif\"\n",
    "X_path = \"../data/processed/X.tif\"\n",
    "\n",
    "# compute_label(\n",
    "#     y_path=y_path,\n",
    "#     plot=plot,\n",
    "#     areas_as_y=False,\n",
    "# )\n",
    "\n",
    "# compute_label(\n",
    "#     y_path=y_area_path,\n",
    "#     plot=plot,\n",
    "#     areas_as_y=True,\n",
    "# )\n",
    "\n",
    "sentinel_composite(\n",
    "    y_path_from=y_path,\n",
    "    X_path_to=X_path,\n",
    "    time_window=(\"2015-07-01\", \"2016-06-30\"),\n",
    "    # num_composites=6,\n",
    "    temporal_reducers=[\"min\", \"max\", \"median\", \"variance\", \"mean\", \"skew\"],\n",
    "    indices=[\"NDVI\", \"NDWI\"],\n",
    "    sentinel_bands=[\"B2\", \"B3\", \"B4\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Interpolate missing values in X, save it as a new raster and visualize it\n",
    "from ltm.data import show_timeseries\n",
    "from ltm.features import load_raster, interpolate_X, save_raster\n",
    "\n",
    "# Define paths and parameters\n",
    "X_path = \"../data/processed/X.tif\"\n",
    "interpolated_X_path = \"../data/processed/interpolated_X.tif\"\n",
    "visualized_reducer = \"min\"\n",
    "rgb_bands = [\"B4\", \"B3\", \"B2\"]\n",
    "\n",
    "# Load, interpolate and save new X\n",
    "X = load_raster(X_path)\n",
    "interpolated_X = interpolate_X(X)\n",
    "save_raster(interpolated_X, X_path, interpolated_X_path)\n",
    "\n",
    "# Visualize\n",
    "show_timeseries(interpolated_X_path, visualized_reducer, rgb_bands=rgb_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "from ltm.features import np2pd_like, load_raster, drop_nan_rows, get_similarity_matrix, show_similarity_matrix, show_dendrogram, dendrogram_dim_red\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (make_scorer, mean_squared_error)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use(\"ltm.mplstyle\")\n",
    "\n",
    "# Load data\n",
    "X_path = \"../data/processed/interpolated_X.tif\"\n",
    "# y_path = \"../data/processed/y.tif\"\n",
    "y_path = \"../data/processed/y_area.tif\"\n",
    "\n",
    "X = load_raster(X_path)\n",
    "y = load_raster(y_path)\n",
    "\n",
    "# Drop NaN, dim red, split data\n",
    "X, y = drop_nan_rows(X, y) # TODO time series imputation (e.g. linear interpolation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # stratified is not really possible, unless every band is binned...\n",
    "\n",
    "# Scale data for non-tree models\n",
    "standard_scaler = StandardScaler().fit(X_train)\n",
    "X_train = standard_scaler.transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "# Make subsets like X and y\n",
    "X_train = np2pd_like(X_train, X)\n",
    "X_test = np2pd_like(X_test, X)\n",
    "y_train = np2pd_like(y_train, y)\n",
    "y_test = np2pd_like(y_test, y)\n",
    "\n",
    "# Show similarity matrix along with dendrogram\n",
    "similarity = get_similarity_matrix(X_train, \"pearson\")\n",
    "sm_ax = show_similarity_matrix(similarity)\n",
    "ddg_ax = show_dendrogram(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim red based on dendrogram with threshold\n",
    "threshold = 0.2\n",
    "\n",
    "X_train = dendrogram_dim_red(X_train, similarity, threshold=threshold)\n",
    "X_test = X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: RFE dim red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "from skelm import ELMRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import (make_scorer, max_error, mean_squared_error,\n",
    "                             median_absolute_error, r2_score)\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from xgboost import XGBRegressor\n",
    "from ltm.models import best_scores, area2mixture_scorer, EndMemberSplitter\n",
    "\n",
    "# Define the models and hyperparameter search space with distributions: expon, gamma, uniform, loguniform or randint\n",
    "# set n_jobs=-1 for parallel processing\n",
    "search_space = {\n",
    "    # ELMRegressor(): {\n",
    "    #     \"alpha\": loguniform(1e-8, 1e5),\n",
    "    #     \"include_original_features\": [True, False],\n",
    "    #     \"n_neurons\": loguniform(1, 100-1),\n",
    "    #     \"ufunc\": [\"tanh\", \"sigm\", \"relu\", \"lin\"],\n",
    "    #     \"density\": uniform(0.01, 0.99),\n",
    "    # },\n",
    "    # XGBRegressor(): {\n",
    "    #     \"n_estimators\": randint(10, 100-10),\n",
    "    #     \"max_depth\": randint(1, 20),\n",
    "    #     \"learning_rate\": uniform(0.01, 0.5),\n",
    "    #     \"gamma\": uniform(0, 0.5),\n",
    "    #     \"min_child_weight\": randint(1, 11),\n",
    "    # },\n",
    "    RandomForestRegressor(n_jobs=-1): {\n",
    "        \"n_estimators\": randint(1, 100-1),\n",
    "        \"max_depth\": randint(1, 20),\n",
    "        \"max_features\": randint(1, 11),\n",
    "        \"min_samples_split\": randint(2, 11),\n",
    "        \"min_samples_leaf\": randint(1, 11),\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"criterion\": [\"squared_error\", \"absolute_error\", \"poisson\", \"friedman_mse\"],\n",
    "    },\n",
    "    # ExtraTreesRegressor(): {\n",
    "    #     \"n_estimators\": randint(1, 100-1),\n",
    "    #     \"min_impurity_decrease\": loguniform(1e-5, 0.5),\n",
    "    #     \"criterion\": [\"squared_error\", \"absolute_error\"],\n",
    "    # },\n",
    "}\n",
    "\n",
    "# Define the scorers and refit (scorer for best model selection)\n",
    "scoring = {\"median_absolute_error\": make_scorer(median_absolute_error,\n",
    "                                                greater_is_better=False),\n",
    "           \"mean_squared_error\": make_scorer(mean_squared_error,\n",
    "                                             greater_is_better=False),\n",
    "           \"coefficient of determination\": make_scorer(r2_score,\n",
    "                                   greater_is_better=False),}\n",
    "refit = \"mean_squared_error\"\n",
    "\n",
    "# Modify scorer incase area labels are used\n",
    "if isinstance(y, pd.DataFrame):\n",
    "    scoring = {scorer_name: area2mixture_scorer(scorer) for scorer_name, scorer in scoring.items()}\n",
    "\n",
    "# Perform hyperparameter search for each model\n",
    "random_state = 0\n",
    "search_results = []\n",
    "for model, param_distributions in search_space.items():\n",
    "    param_distributions[\"random_state\"] = [random_state]\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions,\n",
    "        scoring=scoring,\n",
    "        refit=refit,\n",
    "        cv=EndMemberSplitter(5, shuffle=True, random_state=random_state),\n",
    "        return_train_score=True,\n",
    "        verbose=1,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    search.fit(X.values, y.values)\n",
    "\n",
    "    search_results.append(search)\n",
    "\n",
    "# Serialize object\n",
    "import dill as pickle # Necessary to serialize lambda functions\n",
    "\n",
    "with open(\"../models/search_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(search_results, f)\n",
    "\n",
    "# Show scores\n",
    "scores = best_scores(search_results, scoring)\n",
    "scores[\"root_mean_squared_error\"] = scores[\"mean_squared_error\"]**0.5\n",
    "scores.to_csv(\"../reports/scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize object\n",
    "import dill as pickle # Necessary to serialize lambda functions\n",
    "\n",
    "with open(\"../models/search_results.pkl\", \"rb\") as f:\n",
    "    search_results = pickle.load(f)\n",
    "\n",
    "X_path = \"../data/processed/interpolated_X.tif\"\n",
    "# y_path = \"../data/processed/y.tif\"\n",
    "y_path = \"../data/processed/y_area.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltm.models import cv_predict\n",
    "\n",
    "orig_plot, pred_plots = cv_predict(\n",
    "    search_results,\n",
    "    X_path,\n",
    "    y_path,\n",
    "    rgb_bands=None,\n",
    "    cv=None,\n",
    "    area2mixture=True,\n",
    "    save_path=\"../reports/figures/predictions.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results[0].best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = search_results[0].best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadleaf = y_test[:, 0]\n",
    "conifer = y_test[:, 1]\n",
    "broadleaf_share_test = broadleaf / (broadleaf + conifer)\n",
    "\n",
    "broadleaf = np.maximum(predicted_y[:, 0], 0)\n",
    "conifer = np.maximum(predicted_y[:, 1], 0)\n",
    "broadleaf_share_pred = broadleaf / (broadleaf + conifer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadleaf_share_pred = predicted_y\n",
    "broadleaf_share_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_of_dermination = r2_score(broadleaf_share_test, broadleaf_share_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_of_dermination  # 0.5776951683455496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(broadleaf_share_test, broadleaf_share_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(broadleaf_share_test, broadleaf_share_pred, \"o\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(y_test, predicted_y, \"o\", alpha=0.1)\n",
    "# plt.plot(y_test[:, 1], predicted_y[:, 1], \"o\", alpha=0.1)\n",
    "# plt.plot(broadleaf_share_test, broadleaf_share_pred, \"o\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and predicted values, one plot per prediction\n",
    "for pred_plot, results in zip(pred_plots, search_results):\n",
    "    x = orig_plot.flatten()\n",
    "    y = pred_plot.flatten()\n",
    "    plt.plot(x, y, \".\", alpha=0.1)\n",
    "    plt.xlabel(\"Original\")\n",
    "    plt.ylabel(results.best_estimator_.__class__.__name__)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impurity based feature importance (bad if high cardinality features with many unique values are present)\n",
    "import numpy as np\n",
    "\n",
    "forest = search_results[2].best_estimator_\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(importances, index=band_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "result = permutation_importance(\n",
    "    regressor, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=band_names)\n",
    "forest_importances = forest_importances[forest_importances >= threshold]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean score decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
