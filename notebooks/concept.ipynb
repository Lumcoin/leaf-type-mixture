{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legit Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composite images from plot and Sentinel-2 data\n",
    "from src.data import sentinel_composite\n",
    "import pandas as pd\n",
    "\n",
    "plot = pd.read_csv(\"../data/processed/plot.csv\")\n",
    "\n",
    "X_path, y_path = sentinel_composite(\n",
    "    plot=plot,\n",
    "    time_window=(\"2015-07-01\", \"2016-06-30\"),\n",
    "    num_composites=6,\n",
    "    level_2a=False,\n",
    "    temporal_reducers=[\"min\", \"max\", \"median\", \"variance\", \"mean\", \"skew\"],\n",
    "    indices=[\"NDVI\", \"NDWI\"],\n",
    "    # sentinel_bands=[\"B2\", \"B3\", \"B4\"],\n",
    "    areas_as_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values in X and save it as a new raster (optional)\n",
    "from src.features import load_multi_band_raster, interpolate_X_and_bands, save_raster\n",
    "\n",
    "X_path = \"../data/processed/X.tif\"\n",
    "interpolated_X_path = \"../data/processed/interpolated_X.tif\"\n",
    "\n",
    "X, band_names = load_multi_band_raster(X_path)\n",
    "interpolated_X, band_names = interpolate_X_and_bands(X, band_names)\n",
    "save_raster(interpolated_X, band_names, X_path, interpolated_X_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time series for a specific temporal reducer\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raster_path = interpolated_X_path\n",
    "reducer = \"Min\"  # Important: title case\n",
    "\n",
    "for i in range(1, 6 + 1):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        X = src.read()\n",
    "        bands = src.descriptions\n",
    "        r_band = f\"{i} {reducer} B4\"\n",
    "        g_band = f\"{i} {reducer} B3\"\n",
    "        b_band = f\"{i} {reducer} B2\"\n",
    "        X = X[[bands.index(b) for b in [r_band, g_band, b_band]]]\n",
    "        X = X.transpose(1, 2, 0)\n",
    "        X /= np.nanmax(X)\n",
    "        plt.imshow(X)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "from src.features import load_multi_band_raster, interpolate_X_and_bands, drop_nan, get_similarity_matrix, show_similarity_matrix, show_dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ltm.mplstyle\")\n",
    "\n",
    "# Load data\n",
    "X_path = \"../data/processed/interpolated_X.tif\"\n",
    "y_path = \"../data/processed/y.tif\"\n",
    "\n",
    "X, band_names = load_multi_band_raster(X_path)\n",
    "y, _ = load_multi_band_raster(y_path)\n",
    "\n",
    "# Prevent warnings for single band y\n",
    "if y.shape[1] == 1:\n",
    "    y = y.ravel()\n",
    "\n",
    "# Drop NaN, split data\n",
    "X, band_names = interpolate_X_and_bands(X, band_names)\n",
    "X, y = drop_nan(X, y) # TODO time series imputation (e.g. linear interpolation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # stratified is not really possible, unless every band is binned...\n",
    "\n",
    "# Preprocess for non-tree models\n",
    "standard_scaler = StandardScaler().fit(X_train)\n",
    "X_train = standard_scaler.transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "# TODO: Correlation/mutual information based dim red; TODO permutation importance\n",
    "# similarity = get_similarity_matrix(X, band_names, \"pearson\")\n",
    "# sm_ax = show_similarity_matrix(similarity)\n",
    "# ddg_ax = show_dendrogram(similarity)\n",
    "\n",
    "# dimred=\"mutual_info\", threshold=0.1\n",
    "# most_similar()  # https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html\n",
    "# plot_pairplot() # Pairplot of most similar features\n",
    "# # Idea: Hierarchial Dim Red based on correlation/mutual information with label band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "from skelm import ELMRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import (make_scorer, max_error, mean_squared_error,\n",
    "                             median_absolute_error, r2_score)\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from xgboost import XGBRegressor\n",
    "from src.models import hyperparam_search, best_scores\n",
    "\n",
    "# Define the models and hyperparameter search space with distributions: expon, gamma, uniform, loguniform or randint\n",
    "search_space = {\n",
    "    ELMRegressor(): {\n",
    "        \"alpha\": loguniform(1e-8, 1e5),\n",
    "        \"include_original_features\": [True, False],\n",
    "        \"n_neurons\": loguniform(1, 100-1),\n",
    "        \"ufunc\": [\"tanh\", \"sigm\", \"relu\", \"lin\"],\n",
    "        \"density\": uniform(0.01, 0.99),\n",
    "    },\n",
    "    XGBRegressor(): {\n",
    "        \"n_estimators\": randint(10, 100-10),\n",
    "        \"max_depth\": randint(1, 20),\n",
    "        \"learning_rate\": uniform(0.01, 0.5),\n",
    "        \"gamma\": uniform(0, 0.5),\n",
    "        \"min_child_weight\": randint(1, 11),\n",
    "    },\n",
    "    RandomForestRegressor(): {\n",
    "        \"n_estimators\": randint(1, 100-1),\n",
    "        \"max_depth\": randint(1, 20),\n",
    "        \"max_features\": randint(1, 11),\n",
    "        \"min_samples_split\": randint(2, 11),\n",
    "        \"min_samples_leaf\": randint(1, 11),\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"criterion\": [\"squared_error\", \"absolute_error\", \"poisson\", \"friedman_mse\"],\n",
    "    },\n",
    "    # ExtraTreesRegressor(): {\n",
    "    #     \"n_estimators\": randint(1, 100-1),\n",
    "    #     \"min_impurity_decrease\": loguniform(1e-5, 0.5),\n",
    "    #     \"criterion\": [\"squared_error\", \"absolute_error\"],\n",
    "    # },\n",
    "}\n",
    "\n",
    "# Define the scorers and refit (scorer for best model selection)\n",
    "scoring = {\n",
    "            # \"max_error\": make_scorer(max_error,\n",
    "            #                                 greater_is_better=False),\n",
    "           \"median_absolute_error\": make_scorer(median_absolute_error,\n",
    "                                                greater_is_better=False),\n",
    "           \"mean_squared_error\": make_scorer(mean_squared_error,\n",
    "                                             greater_is_better=False),\n",
    "           \"coefficient of determination\": make_scorer(r2_score,\n",
    "                                   greater_is_better=False),}\n",
    "refit = \"mean_squared_error\"\n",
    "\n",
    "search_results = hyperparam_search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    search_space,\n",
    "    scoring,\n",
    "    refit,\n",
    "    kfold_from_endmembers=True,\n",
    "    kfold_n_splits=5,\n",
    "    kfold_n_iter=10,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# Serialize object\n",
    "import dill as pickle # Necessary to serialize lambda functions\n",
    "\n",
    "with open(\"search_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(search_results, f)\n",
    "\n",
    "# Show scores\n",
    "scores = best_scores(search_results, scoring)\n",
    "scores[\"root_mean_squared_error\"] = scores[\"mean_squared_error\"]**0.5\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize object\n",
    "import dill as pickle # Necessary to serialize lambda functions\n",
    "\n",
    "with open(\"search_results.pkl\", \"rb\") as f:\n",
    "    search_results = pickle.load(f)\n",
    "\n",
    "X_path = \"../data/processed/interpolated_X.tif\"\n",
    "y_path = \"../data/processed/y.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import cv_predict\n",
    "\n",
    "orig_plot, pred_plots = cv_predict(\n",
    "    search_results, \n",
    "    X_path, \n",
    "    y_path, \n",
    "    # rgb_bands=None,\n",
    "    kfold_from_endmembers=False,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results[2].best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = search_results[2].best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadleaf = y_test[:, 0]\n",
    "conifer = y_test[:, 1]\n",
    "broadleaf_share_test = broadleaf / (broadleaf + conifer)\n",
    "\n",
    "broadleaf = np.maximum(predicted_y[:, 0], 0)\n",
    "conifer = np.maximum(predicted_y[:, 1], 0)\n",
    "broadleaf_share_pred = broadleaf / (broadleaf + conifer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadleaf_share_pred = predicted_y\n",
    "broadleaf_share_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_of_dermination = r2_score(broadleaf_share_test, broadleaf_share_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_of_dermination  # 0.5776951683455496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(broadleaf_share_test, broadleaf_share_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(broadleaf_share_test, broadleaf_share_pred, \"o\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(y_test, predicted_y, \"o\", alpha=0.1)\n",
    "# plt.plot(y_test[:, 1], predicted_y[:, 1], \"o\", alpha=0.1)\n",
    "# plt.plot(broadleaf_share_test, broadleaf_share_pred, \"o\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and predicted values, one plot per prediction\n",
    "for pred_plot, results in zip(pred_plots, search_results):\n",
    "    x = orig_plot.flatten()\n",
    "    y = pred_plot.flatten()\n",
    "    plt.plot(x, y, \".\", alpha=0.1)\n",
    "    plt.xlabel(\"Original\")\n",
    "    plt.ylabel(results.best_estimator_.__class__.__name__)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impurity based feature importance (bad if high cardinality features with many unique values are present)\n",
    "import numpy as np\n",
    "\n",
    "forest = search_results[2].best_estimator_\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(importances, index=band_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    search_results[2].best_estimator_, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=band_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean score decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
