{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 12,\n",
       " 'min': 12,\n",
       " 'max': 12,\n",
       " 'mean': 11,\n",
       " 'median': 10,\n",
       " 'kurtosis': 5,\n",
       " 'kendallsCorrelation': 3,\n",
       " 'sampleVariance': 2,\n",
       " 'skew': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get optimal composite values\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../reports/reducer_composites.csv\")\n",
    "metric = \"Root Mean Squared Error\"\n",
    "\n",
    "optimal_idx = df.groupby(\"Reducer\")[metric].idxmin()\n",
    "optimal_df = df.loc[optimal_idx]\n",
    "optimal_df = optimal_df.set_index(\"Reducer\")\n",
    "\n",
    "reducers = [\n",
    "    \"median\",\n",
    "    \"mean\",\n",
    "    \"mode\",\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "    \"sampleVariance\",\n",
    "    \"kendallsCorrelation\",\n",
    "    \"skew\",\n",
    "    \"kurtosis\",\n",
    "]\n",
    "\n",
    "optimal_df = optimal_df.loc[reducers]\n",
    "composite_dict = optimal_df[\"Composites\"].to_dict()\n",
    "\n",
    "composite_dict = {k: v for k, v in sorted(composite_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "composite_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Earth Engine API...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['B9', 'WVP', 'TCI_G'],\n",
       " ['BAIM',\n",
       "  'BCC',\n",
       "  'BRBA',\n",
       "  'BWDRVI',\n",
       "  'GCC',\n",
       "  'GVMI',\n",
       "  'MCARI705',\n",
       "  'MGRVI',\n",
       "  'MIRBI',\n",
       "  'MLSWI26',\n",
       "  'MNLI',\n",
       "  'MTCI',\n",
       "  'NBRplus',\n",
       "  'NDCI',\n",
       "  'NDDI',\n",
       "  'NDGI',\n",
       "  'S2WI',\n",
       "  'SIPI',\n",
       "  'TRRVI',\n",
       "  'TTVI',\n",
       "  'VIBI',\n",
       "  'WI2015',\n",
       "  'kEVI',\n",
       "  'kIPVI',\n",
       "  'kNDVI',\n",
       "  'kVARI',\n",
       "  'mND705',\n",
       "  'mSR705'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the optimal bands\n",
    "from ltm.models import bands_from_importance\n",
    "\n",
    "sentinel_bands, indices = bands_from_importance(\n",
    "    \"../reports/band_importance.csv\"\n",
    ")\n",
    "\n",
    "sentinel_bands, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create data for one year\n",
    "from datetime import datetime\n",
    "from ltm.data import sentinel_composite, split_band_name\n",
    "from ltm.features import load_raster, interpolate_data, to_float32, save_raster\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def create_data(year, target_path, data_folder, batch_size=None):\n",
    "    # Skip if data already exists\n",
    "    stem = Path(data_folder).stem\n",
    "    data_path = Path(data_folder) / f\"{year}/{stem}.tif\"\n",
    "    if data_path.exists():\n",
    "        return\n",
    "    data_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create one composite for each reducer\n",
    "    data_path = str(data_path)\n",
    "    total_data = pd.DataFrame()\n",
    "    for reducer, num_composites in tqdm(\n",
    "        composite_dict.items(), desc=f\"Downloading Composites for {year}\"\n",
    "    ):\n",
    "        composite_path = str(\n",
    "            Path(data_folder) / f\"{year}/{stem}_{reducer}_{num_composites}.tif\"\n",
    "        )\n",
    "\n",
    "        sleep_time = 60\n",
    "        while not Path(composite_path).exists():\n",
    "            try:\n",
    "                sentinel_composite(\n",
    "                    target_path,\n",
    "                    composite_path,\n",
    "                    time_window=(\n",
    "                        datetime(year, 4, 1),\n",
    "                        datetime(year + 1, 4, 1),\n",
    "                    ),\n",
    "                    num_composites=num_composites,\n",
    "                    temporal_reducers=[reducer],\n",
    "                    indices=indices,\n",
    "                    sentinel_bands=sentinel_bands,\n",
    "                    batch_size=batch_size,\n",
    "                )\n",
    "            except BaseException as e:\n",
    "                print(e)\n",
    "                sleep(sleep_time)  # sleep for five minutes\n",
    "                sleep_time *= 2\n",
    "    \n",
    "    for reducer, num_composites in tqdm(\n",
    "        composite_dict.items(), desc=f\"Combining Composites for {year}\"\n",
    "    ):\n",
    "        composite_path = str(\n",
    "            Path(data_folder) / f\"{year}/{stem}_{reducer}_{num_composites}.tif\"\n",
    "        )\n",
    "\n",
    "        # Combine into one raster\n",
    "        data = load_raster(composite_path)\n",
    "        data = interpolate_data(data)\n",
    "        columns = [\n",
    "            column\n",
    "            for column in data.columns\n",
    "            if split_band_name(column)[2:] != (\"kendallsCorrelation\", \"p-value\")\n",
    "        ]\n",
    "        data = data[columns]\n",
    "        data = to_float32(data)\n",
    "        total_data = pd.concat([total_data, data], axis=1)\n",
    "\n",
    "    # Save the concatenated data\n",
    "    save_raster(total_data, target_path, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for each year\n",
    "target_path = \"../data/processed/target.tif\"\n",
    "data_folder = \"../data/processed/generalization/data/\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]#, 2023]\n",
    "\n",
    "for year in tqdm(years, desc=\"Years\"):\n",
    "    create_data(year, target_path, data_folder)\n",
    "\n",
    "# Combine all data into one dataframe\n",
    "print(\"Combining data...\")\n",
    "total_data = pd.DataFrame()\n",
    "for year in tqdm(years, desc=\"Years\"):\n",
    "    stem = Path(data_folder).stem\n",
    "    data_path = Path(data_folder) / f\"{year}/{stem}.tif\"\n",
    "    data = load_raster(str(data_path))\n",
    "    total_data = pd.concat([total_data, data])\n",
    "\n",
    "# Create target data\n",
    "target = load_raster(target_path)\n",
    "total_target = pd.concat([target] * len(years))\n",
    "\n",
    "# Drop rows with NaN label\n",
    "data, target = total_data[total_target.notna()], total_target[total_target.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on new data of the study area\n",
    "import dill\n",
    "from sklearn.base import clone\n",
    "\n",
    "df = pd.read_csv(\"../reports/hyperparameter_tuning.csv\", index_col=0)\n",
    "best_model = df[\"Root Mean Squared Error\"].idxmin()\n",
    "\n",
    "with open(f\"../models/{best_model}.pkl\", \"rb\") as f:\n",
    "    model = dill.load(f)\n",
    "\n",
    "# Clone model in case warm_start=True and fit on new data\n",
    "model = clone(model)\n",
    "model.fit(data, target)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_validate\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "# cv_results = cross_validate(model, data, target, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, return_train_score=True)\n",
    "# # TODO: better custom CV splitter needed -> separate same pixel completely (7 times)\n",
    "\n",
    "# cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(-cv_results[\"test_score\"].mean())\n",
    "\n",
    "# raise ValueError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold, BaseCrossValidator\n",
    "\n",
    "# class RepeatedKFold(BaseCrossValidator):\n",
    "#     def __init__(self, n_splits=5, n_repeats=1, shuffle=False, random_state=None):\n",
    "#         self.n_splits = n_splits\n",
    "#         self.n_repeats = n_repeats\n",
    "#         self.k_fold = KFold(\n",
    "#             n_splits=n_splits, shuffle=shuffle, random_state=random_state\n",
    "#         )\n",
    "\n",
    "#     def split(self, X, y, groups=None):\n",
    "#         X = np.array(X)\n",
    "#         y = np.array(y)\n",
    "\n",
    "#         if y.shape[0] % self.n_repeats != 0:\n",
    "#             raise ValueError(\n",
    "#                 \"The number of samples should be divisible by n_repeats.\"\n",
    "#             )\n",
    "#         length = y.shape[0] // self.n_repeats\n",
    "#         for train, test in self.k_fold.split(X[:length], y[:length]):\n",
    "#             repeated_train = np.concatenate(\n",
    "#                 [train + i * length for i in range(self.n_repeats)]\n",
    "#             )\n",
    "#             repeated_test = np.concatenate(\n",
    "#                 [test + i * length for i in range(self.n_repeats)]\n",
    "#             )\n",
    "\n",
    "#             yield repeated_train, repeated_test\n",
    "\n",
    "#     def get_n_splits(self, X = None, y = None, groups = None):\n",
    "#         return self.n_splits\n",
    "\n",
    "# cv = RepeatedKFold(n_splits=5, n_repeats=len(years), shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# cv_results = cross_validate(model, data, target, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# print(cv_results[\"test_score\"].mean())  # 0.2828385271167998\n",
    "# # tuned ELM has 0.280xxx Cross Validation Score\n",
    "# cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create save folder and wrapper functions\n",
    "# from pathlib import Path\n",
    "# from sklearn.metrics import make_scorer, root_mean_squared_error\n",
    "\n",
    "# save_folder = \"./\"\n",
    "# Path(save_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Define the metric\n",
    "# rmse_scorer = make_scorer(\n",
    "#     root_mean_squared_error, greater_is_better=False\n",
    "# )\n",
    "\n",
    "\n",
    "# def suggest_categorical(*args, **kwargs):\n",
    "#     return \"suggest_categorical\", args, kwargs\n",
    "\n",
    "\n",
    "# def suggest_discrete_uniform(*args, **kwargs):\n",
    "#     return \"suggest_discrete_uniform\", args, kwargs\n",
    "\n",
    "\n",
    "# def suggest_float(*args, **kwargs):\n",
    "#     return \"suggest_float\", args, kwargs\n",
    "\n",
    "\n",
    "# def suggest_int(*args, **kwargs):\n",
    "#     return \"suggest_int\", args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skelm import ELMRegressor\n",
    "# from ltm.models import hyperparam_search\n",
    "\n",
    "# elm_default = ELMRegressor(random_state=42)\n",
    "# search_space = [\n",
    "#     suggest_float(\"alpha\", 1e-8, 1e5, log=True),\n",
    "#     suggest_categorical(\"include_original_features\", [True, False]),\n",
    "#     suggest_float(\"n_neurons\", 1, 1000),\n",
    "#     suggest_categorical(\"ufunc\", [\"tanh\", \"sigm\", \"relu\", \"lin\"]),\n",
    "#     suggest_float(\"density\", 0.01, 0.99),\n",
    "# ]\n",
    "\n",
    "# elm_model, elm_study = hyperparam_search(\n",
    "#     elm_default,\n",
    "#     search_space,\n",
    "#     data,\n",
    "#     target,\n",
    "#     rmse_scorer,\n",
    "#     cv=cv,\n",
    "#     n_trials=500,\n",
    "#     save_folder=save_folder,\n",
    "#     random_state=42,\n",
    "# )\n",
    "# elm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-val predict on seen data\n",
    "\n",
    "# Prediction on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty target raster from shape with NaN outside of shape\n",
    "from ltm.data import shapefile2raster\n",
    "from pathlib import Path\n",
    "\n",
    "name = \"Brunnstube\"\n",
    "shapefile_path = f\"../data/raw/{name}/{name}.shp\"\n",
    "target_path = f\"../data/processed/generalization/{name}.tif\"\n",
    "year = 2023\n",
    "batch_size = (\n",
    "    100  # 25 for Freisinger Forst, 100 for Peterfecking, 200 for Traunstein, None for Brunnstube\n",
    ")\n",
    "\n",
    "Path(target_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "if not Path(target_path).exists():\n",
    "    shapefile2raster(\n",
    "        shapefile_path=shapefile_path,\n",
    "        raster_path=target_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m stem \u001b[38;5;241m=\u001b[39m Path(target_path)\u001b[38;5;241m.\u001b[39mstem\n\u001b[0;32m      3\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path(target_path)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m stem)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcreate_data\u001b[49m(year, target_path, data_folder, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Create data for the target area\n",
    "stem = Path(target_path).stem\n",
    "data_folder = str(Path(target_path).parent / stem)\n",
    "create_data(year, target_path, data_folder, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "data_path = str(Path(data_folder) / f\"{year}/{stem}.tif\")\n",
    "data = load_raster(data_path)\n",
    "\n",
    "xgb_pred = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prediction by overwriting the shape raster\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "with rasterio.open(target_path) as src:\n",
    "    profile = src.profile\n",
    "    shape = src.read().shape\n",
    "    nan_mask = np.isnan(src.read())\n",
    "\n",
    "xgb_reshaped = xgb_pred.reshape(shape)\n",
    "\n",
    "with rasterio.open(target_path, \"w\", **profile) as dst:\n",
    "    xgb_reshaped[nan_mask] = np.nan\n",
    "    dst.write(xgb_reshaped)\n",
    "    dst.descriptions = (\"Conifer Proportion\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use science style\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.style.use(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "norm = Normalize(vmin=0, vmax=1)\n",
    "plt.imshow(\n",
    "    xgb_reshaped.transpose(1, 2, 0),\n",
    "    cmap=\"viridis\",\n",
    "    norm=norm,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plot the distribution of the recording dates\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "ax = sns.kdeplot(xgb_reshaped.flatten())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
